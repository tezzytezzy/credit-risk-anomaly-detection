{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'local[*]' - instructing to use all the cores (*) on a local machine\n",
    "# 'appName' - will be given a random name unless specified, like Docker container image name\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "        master('local[*]').\\\n",
    "        appName('credit risk').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source CSV file must reside in the same folder\n",
    "# The 'german.data-numeric' file delimits columns with 2 spaces ('  ')\n",
    "#   Without data scrubbing, \"IllegalArgumentException: 'Delimiter cannot be more than one character:   '\"\n",
    "df = spark.read.csv('german.data',sep=' ',inferSchema=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 21)\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      "\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_c0|_c1|_c2|_c3| _c4|_c5|_c6|_c7|_c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|   4|A121|  67|A143|A152|   2|A173|   1|A192|A201|   1|\n",
      "|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|   2|A121|  22|A143|A152|   1|A173|   1|A191|A201|   2|\n",
      "|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|   3|A121|  49|A143|A152|   1|A172|   2|A191|A201|   1|\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_ROW_NUM = 3\n",
    "COL_TOTAL_NUM = len(df.columns)\n",
    "\n",
    "print((df.count(),COL_TOTAL_NUM))\n",
    "df.printSchema()\n",
    "df.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the columns with numbers are \"**ordinal**\" i.e., the magnitude in each column/feature means \"something\" (e.g., age, duration, credit amount and installment rate). Thus, **transformation** and **encoding** is NOT needed and they will be converted to non-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ShortType # a signed 16-bit integer vs. IntegerType (a signed 32-bit integer)\n",
    "\n",
    "col_indices_of_ordinal_numbers = [2, 5, 8, 11, 13, 16, 18] # Ignore the target/label column i.e., '_c20', the last column\n",
    "col_indices_of_categorical_numbers = []\n",
    "\n",
    "def convert_integer_to_string_in_list(aList: list) -> list:\n",
    "    return [str(item) for item in aList]\n",
    "\n",
    "for col_idx in range(1, COL_TOTAL_NUM):\n",
    "    if col_idx not in col_indices_of_ordinal_numbers:\n",
    "        col_indices_of_categorical_numbers.append(col_idx)\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # change the name of each column e.g., \"_c0\" -> \"1\"\n",
    "    new_col_num = int(col_name.replace('_c', '')) + 1\n",
    "    df = df.withColumnRenamed(col_name, str(new_col_num))\n",
    "    \n",
    "    if new_col_num in col_indices_of_ordinal_numbers:  \n",
    "        df = df.withColumn(str(new_col_num), df[str(new_col_num)].cast(ShortType()))\n",
    "        \n",
    "col_indices_of_ordinal_numbers = convert_integer_to_string_in_list(col_indices_of_ordinal_numbers)\n",
    "col_indices_of_categorical_numbers = convert_integer_to_string_in_list(col_indices_of_categorical_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|\n",
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|\n",
      "|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|\n",
      "|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|\n",
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'string'),\n",
       " ('2', 'smallint'),\n",
       " ('3', 'string'),\n",
       " ('4', 'string'),\n",
       " ('5', 'smallint'),\n",
       " ('6', 'string'),\n",
       " ('7', 'string'),\n",
       " ('8', 'smallint'),\n",
       " ('9', 'string'),\n",
       " ('10', 'string'),\n",
       " ('11', 'smallint'),\n",
       " ('12', 'string'),\n",
       " ('13', 'smallint'),\n",
       " ('14', 'string'),\n",
       " ('15', 'string'),\n",
       " ('16', 'smallint'),\n",
       " ('17', 'string'),\n",
       " ('18', 'smallint'),\n",
       " ('19', 'string'),\n",
       " ('20', 'string'),\n",
       " ('21', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(PRINT_ROW_NUM)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Databricks, the leading crowd platform for massive scale data engineering and collaborative data science with optimized Apache Sparkâ„¢ clusters on AWS or Azure, just pubslihed [here](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5703650563260092/2738738457173388/3024994340111770/latest.html) and [there](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7274796508260238/2844662950865680/2879908466733289/latest.html) on Apache Spark MLlib Pipelines API. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "COL_SUFFIX_IDX = \"_INDEX\"\n",
    "COL_SUFFIX_VEC = \"_VEC\"\n",
    "\n",
    "# categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "stages = [] # stages in our Pipeline\n",
    "for col_idx in col_indices_of_categorical_numbers:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=col_idx, outputCol=col_idx + COL_SUFFIX_IDX)\n",
    "    \n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[col_idx + COL_SUFFIX_VEC])\n",
    "    \n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumnRenamed(str(len(df.columns)), LABEL_COL_NAME)\n",
    "\n",
    "LABEL_COL_NAME = \"label\"\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol=str(COL_TOTAL_NUM), outputCol=LABEL_COL_NAME)\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "FEATURE_VECTOR_COL_NAME = \"features\"\n",
    "\n",
    "assemblerInputs = [col_idx + COL_SUFFIX_IDX for col_idx in col_indices_of_categorical_numbers] + col_indices_of_ordinal_numbers\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=FEATURE_VECTOR_COL_NAME)\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "STANDARDIZED_FEATURES_COL_NAME = 'std_features'\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol=FEATURE_VECTOR_COL_NAME, outputCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "stages += [standardScaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(df)\n",
    "df_pipeline = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "def print_performance_metrics(predictions):\n",
    "  # Evaluate model\n",
    "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "  auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "  aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "  print(\"auc = {}\".format(auc))\n",
    "  print(\"aupr = {}\".format(aupr))\n",
    "\n",
    "  # get rdd of predictions and labels for mllib eval metrics\n",
    "  predictionAndLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "\n",
    "  # Instantiate metrics objects\n",
    "  binary_metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "  multi_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "  # Area under precision-recall curve\n",
    "  print(\"Area under PR = {}\".format(binary_metrics.areaUnderPR))\n",
    "  # Area under ROC curve\n",
    "  print(\"Area under ROC = {}\".format(binary_metrics.areaUnderROC))\n",
    "  # Accuracy\n",
    "  print(\"Accuracy = {}\".format(multi_metrics.accuracy))\n",
    "  # Confusion Matrix\n",
    "  print(multi_metrics.confusionMatrix())\n",
    "  \n",
    "  ### Question 5.1 Answer ###\n",
    "  \n",
    "  # F1\n",
    "  print(\"F1 = {}\".format(multi_metrics.fMeasure()))\n",
    "  # Precision\n",
    "  print(\"Precision = {}\".format(multi_metrics.precision()))\n",
    "  # Recall\n",
    "  print(\"Recall = {}\".format(multi_metrics.recall()))\n",
    "  # FPR\n",
    "  print(\"FPR = {}\".format(multi_metrics.falsePositiveRate(0.0)))\n",
    "  # TPR\n",
    "  print(\"TPR = {}\".format(multi_metrics.truePositiveRate(0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11832168009876139\n",
      "0.2759429720647131\n",
      "0.4068048276404337\n",
      "0.5212579515868058\n",
      "0.620595299353551\n",
      "0.71070821099644\n",
      "0.7946167029948367\n",
      "0.8697509575845473\n",
      "0.936440526700285\n",
      "0.9871407977706779\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PCA_FEATURES_COL_NAME = \"pca_features\"\n",
    "PCA_feature_num_list = np.arange(1, COL_TOTAL_NUM, 2)\n",
    "PCA_explainedVariances_list = []\n",
    "cumulative_explained_variance = 0\n",
    "\n",
    "for PCA_feature_num in PCA_feature_num_list:\n",
    "    PCA_extracted = PCA(k=PCA_feature_num, inputCol=STANDARDIZED_FEATURES_COL_NAME, outputCol=PCA_FEATURES_COL_NAME)\n",
    "\n",
    "    pca_model = PCA_extracted.fit(df_pipeline)\n",
    "    df_PCA = pca_model.transform(df_pipeline)\n",
    "\n",
    "    df_train, df_test = df_PCA.randomSplit([0.7, 0.3], seed = 100)\n",
    "    \n",
    "    LR = LogisticRegression(labelCol=LABEL_COL_NAME, featuresCol=PCA_FEATURES_COL_NAME, maxIter=50)\n",
    "    LR_model = LR.fit(df_train)\n",
    "    LR_preds = LR_model.transform(df_test) # Scikit-Learn uses predict() method for prediction\n",
    "#     LR_preds.select(LABEL_COL_NAME, \"prediction\", \"rawPrediction\", \"probability\").show(PRINT_ROW_NUM)\n",
    "    \n",
    "#     print(pca_model.explainedVariance) [0.11832168009876139,0.08249816430977125,0.07512312765618046]\n",
    "#     print(np.cumsum(pca_model.explainedVariance)) [0.11832168 0.20081984 0.27594297]\n",
    "#     cumulative_explained_variance = cumulative_explained_variance + np.cumsum(pca_model.explainedVariance)[PCA_feature_num - 1]\n",
    "#     PCA_explainedVariances_list.append(cumulative_explained_variance)\n",
    "    PCA_explainedVariances_list.append(np.cumsum(pca_model.explainedVariance)[PCA_feature_num - 1])\n",
    "    print(np.cumsum(pca_model.explainedVariance)[PCA_feature_num - 1])\n",
    "#     print_performance_metrics(LR_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "ax.plot(PCA_feature_num_list, PCA_explainedVariances_list)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty apparent that ALL the features should be used in making prediction, as each component noteciably adds variance in relation to the total variance - almost in a straight-line fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "|label|            features|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|        std_features|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "|  0.0|[1.0,1.0,0.0,1.0,...|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|[-0.0010448944788...|\n",
      "|  1.0|(20,[0,5,7,13,14,...|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|[1.04384958436457...|\n",
      "|  0.0|(20,[1,2,4,7,10,1...|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|[-1.0459393733222...|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardise the data by scaling the features to have zero mean and unit standard deviation\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "STANDARDIZED_FEATURES_COL_NAME = 'std_features'\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol=FEATURE_VECTOR_COL_NAME, outputCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "standardScaler_model = standardScaler.fit(df_pipeline)\n",
    "df_final_standardized = standardScaler_model.transform(df_pipeline)\n",
    "df_final_standardized.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "\n",
    "#RAW_FEATURES = \"raw_feasures\"\n",
    "PCA_FEATURES_COL_NAME = \"pca_features\"\n",
    "PCA_FEATURES_NUM = int(COL_TOTAL_NUM / 2)\n",
    "\n",
    "#df_pca = spark.createDataFrame(features_in_densevector,[RAW_FEATURES])\n",
    "#type(df_pca) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "#df_pca.printSchema()\n",
    "PCA_extracted = PCA(k=PCA_FEATURES_NUM, inputCol=STANDARDIZED_FEATURES_COL_NAME, outputCol=PCA_FEATURES_COL_NAME)\n",
    "\n",
    "stages += [PCA_extracted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the stages as a Pipeline.\n",
    "# This puts the data through all of the feature transformations we described in a single call.\n",
    "  \n",
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(df)\n",
    "df_pipeline = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "315\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df_pipeline.randomSplit([0.7, 0.3], seed = 100)\n",
    "print (df_train.count())\n",
    "print (df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, 1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, 21: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### IS THIS NECESSARY?????\n",
    "\n",
    "# # Keep relevant columns \n",
    "# # label column is the transformation of Income\n",
    "# # Features are the numberical representation of all the data numerical+categorical. \n",
    "# selected_cols = [LABEL_COL_NAME, FEATURE_VECTOR_COL_NAME] + df.columns\n",
    "# df_pipeline = df_pipeline.select(selected_cols)\n",
    "# df_pipeline.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "|label|            features|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|        std_features|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "|  0.0|[1.0,1.0,0.0,1.0,...|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|[-0.0010448944788...|\n",
      "|  1.0|(20,[0,5,7,13,14,...|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|[1.04384958436457...|\n",
      "|  0.0|(20,[1,2,4,7,10,1...|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|[-1.0459393733222...|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardise the data by scaling the features to have zero mean and unit standard deviation\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "STANDARDIZED_FEATURES_COL_NAME = 'std_features'\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol=FEATURE_VECTOR_COL_NAME, outputCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "standardScaler_model = standardScaler.fit(df_pipeline)\n",
    "df_final_standardized = standardScaler_model.transform(df_pipeline)\n",
    "df_final_standardized.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "315\n"
     ]
    }
   ],
   "source": [
    "# Randomly split data into training and test sets. set seed for reproducibility\n",
    "# Simialr to sklearn test_train_split\n",
    "df_train, df_test = df_final_standardized.randomSplit([0.7, 0.3], seed = 100)\n",
    "print (df_train.count())\n",
    "print (df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** STANDARDIZED FEATURES ***\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|label|prediction|       rawPrediction|         probability|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|  1.0|       0.0|[1.98878882577249...|[0.87961494222793...|\n",
      "|  0.0|       0.0|[2.59363122276489...|[0.93045056953822...|\n",
      "|  0.0|       0.0|[2.65212943642400...|[0.93414211593761...|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "DenseMatrix([[ 0.40849242,  0.15921003,  0.10444606, -0.35825602,  0.10119409,\n",
      "               0.32339495,  0.13529649,  0.16577387,  0.199668  ,  0.16229824,\n",
      "              -0.0841887 , -0.22727041, -0.25552495,  0.27869821,  0.20885488,\n",
      "               0.36299363,  0.01800106, -0.23550216, -0.04776179,  0.10293513]])\n",
      "Accuracy: 0.7313868613138687\n",
      "False positive (FP) rate: 0.5056232356373986\n",
      "True positive (TP) rate: 0.7313868613138687\n",
      "Recall: 0.7313868613138687\n",
      "Precision: 0.7093861144728328\n",
      "\n",
      "\n",
      "*** PCA-SELECTED FEATURES ***\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|label|prediction|       rawPrediction|         probability|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|  1.0|       0.0|[1.78128211581802...|[0.85585510904920...|\n",
      "|  0.0|       0.0|[1.75759183138588...|[0.85290779747608...|\n",
      "|  0.0|       0.0|[2.83314045213547...|[0.94444061974445...|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "DenseMatrix([[-0.20532344,  0.34605834, -0.09899167, -0.00437557,  0.06953392,\n",
      "               0.65440333, -0.28637784, -0.09768004,  0.11710577, -0.00641603]])\n",
      "Accuracy: 0.7182481751824817\n",
      "False positive (FP) rate: 0.5389405561541663\n",
      "True positive (TP) rate: 0.7182481751824817\n",
      "Recall: 0.7182481751824817\n",
      "Precision: 0.6902125843941309\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def run_logistic_regression(feature_vector_col_name: 'str'):\n",
    "    def print_regressor_summary(regressorModel: 'LogisticRegressionModel'):\n",
    "        regressor_summary=regressorModel.summary\n",
    "\n",
    "        print(regressorModel.coefficientMatrix)\n",
    "\n",
    "        # The ratio of both True-True and False-False in all instances\n",
    "        print(\"Accuracy: \" + str(regressor_summary.accuracy))\n",
    "\n",
    "        # Predicted True but Actual False :-(\n",
    "        print(\"False positive (FP) rate: \" + str(regressor_summary.weightedFalsePositiveRate))\n",
    "\n",
    "        # Predicted True with Actual True :-)\n",
    "        print(\"True positive (TP) rate: \" + str(regressor_summary.weightedTruePositiveRate))\n",
    "\n",
    "\n",
    "        print(\"Recall: \" + str(regressor_summary.weightedRecall)) # TP / (TP + FP)\n",
    "        print(\"Precision: \" + str(regressor_summary.weightedPrecision)) # TP / (TP + False Negative (FN))\n",
    "\n",
    "        # print(\"F-measure: \" + str(LR_summary.weightedFMeasure))    \n",
    "\n",
    "    LR = LogisticRegression(labelCol=LABEL_COL_NAME, featuresCol=feature_vector_col_name, maxIter=50)\n",
    "    LR_model = LR.fit(df_train)\n",
    "    LR_preds = LR_model.transform(df_test) # Scikit-Learn uses predict() method for prediction\n",
    "    LR_preds.select(LABEL_COL_NAME, \"prediction\", \"rawPrediction\", \"probability\").show(PRINT_ROW_NUM)\n",
    "    \n",
    "    print_regressor_summary(LR_model)\n",
    "\n",
    "print(\"*** STANDARDIZED FEATURES ***\")\n",
    "run_logistic_regression(STANDARDIZED_FEATURES_COL_NAME)\n",
    "print(\"\\n\\n*** PCA-SELECTED FEATURES ***\")\n",
    "run_logistic_regression(PCA_FEATURES_COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 0.42929465,  0.1401096 , -0.03364896, -0.43922751, -0.00654812,\n",
      "               0.21920053,  0.10082188,  0.09989184,  0.23796936,  0.20468812,\n",
      "               0.01633952, -0.23483937, -0.24732628,  0.17157324,  0.26937861,\n",
      "               0.30199176,  0.06606606, -0.32354628, -0.0647771 , -0.01753075]])\n",
      "Accuracy: 0.7226277372262774\n",
      "False positive (FP) rate: 0.5149235801028204\n",
      "True positive (TP) rate: 0.7226277372262774\n",
      "Recall: 0.7226277372262774\n",
      "Precision: 0.6976008770664435\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "def print_performance_metrics(predictions):\n",
    "  # Evaluate model\n",
    "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "  auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "  aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "  print(\"auc = {}\".format(auc))\n",
    "  print(\"aupr = {}\".format(aupr))\n",
    "\n",
    "  # get rdd of predictions and labels for mllib eval metrics\n",
    "  predictionAndLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "\n",
    "  # Instantiate metrics objects\n",
    "  binary_metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "  multi_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "  # Area under precision-recall curve\n",
    "  print(\"Area under PR = {}\".format(binary_metrics.areaUnderPR))\n",
    "  # Area under ROC curve\n",
    "  print(\"Area under ROC = {}\".format(binary_metrics.areaUnderROC))\n",
    "  # Accuracy\n",
    "  print(\"Accuracy = {}\".format(multi_metrics.accuracy))\n",
    "  # Confusion Matrix\n",
    "  print(multi_metrics.confusionMatrix())\n",
    "  \n",
    "  ### Question 5.1 Answer ###\n",
    "  \n",
    "  # F1\n",
    "  print(\"F1 = {}\".format(multi_metrics.fMeasure()))\n",
    "  # Precision\n",
    "  print(\"Precision = {}\".format(multi_metrics.precision()))\n",
    "  # Recall\n",
    "  print(\"Recall = {}\".format(multi_metrics.recall()))\n",
    "  # FPR\n",
    "  print(\"FPR = {}\".format(multi_metrics.falsePositiveRate(0.0)))\n",
    "  # TPR\n",
    "  print(\"TPR = {}\".format(multi_metrics.truePositiveRate(0.0)))\n",
    "  \n",
    "  \n",
    "print_performance_metrics(lrPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.sql import sqlContext\n",
    "#from pyspark.mllib.utils import MLUtils\n",
    "#from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "#RAW_FEATURES = \"raw_feasures\"\n",
    "PCA_FEATURES_COL_NAME = \"pca_features\"\n",
    "PCA_FEATURES_NUM = int(COL_TOTAL_NUM / 2)\n",
    "\n",
    "#df_pca = spark.createDataFrame(features_in_densevector,[RAW_FEATURES])\n",
    "#type(df_pca) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "#df_pca.printSchema()\n",
    "PCA_extracted = PCA(k=PCA_FEATURES_NUM, inputCol=STANDARDIZED_FEATURES_COL_NAME, outputCol=PCA_FEATURES_COL_NAME)\n",
    "#type(pca_extracted) pyspark.ml.feature.PCA\n",
    "\n",
    "PCA_model = pca_extracted.fit(df_final_standardized)\n",
    "#type(model) pyspark.ml.feature.PCAModel\n",
    "\n",
    "df_PCA = PCA_model.transform(df_final_standardized) # this create a DataFrame with the regular features and pca_features\n",
    "#type(features) pyspark.sql.dataframe.DataFrame\n",
    "features.collect()[0][PCA_FEATURES]\n",
    "\n",
    "# # We can now extract the pca_features to prepare our RowMatrix.\n",
    "pca_features = df_PCA.select(PCA_FEATURES).rdd.map(lambda row : row[0])\n",
    "#type(pca_features) # pyspark.rdd.PipelinedRDD\n",
    "\n",
    "\n",
    "#mat = RowMatrix(pca_features)\n",
    "#type(mat) # pyspark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "#svd_model = RowMatrix(pca_features).computeSVD(PCA_FEATURES_NUM, True)\n",
    "#svd_model = mat.computeSVD(PCA_FEATURES_NUM, True)\n",
    "# svd_model.U.rows.collect()\n",
    "# # [DenseVector([-0.7071, 0.7071]), DenseVector([-0.7071, -0.7071])]\n",
    "\n",
    "# svd_model.s\n",
    "# # DenseVector([3.4641, 3.1623])\n",
    "\n",
    "# svd_model.V\n",
    "# # DenseMatrix(3, 2, [-0.4082, -0.8165, -0.4082, 0.8944, -0.4472, 0.0], 0)\n",
    "\n",
    "\n",
    "# # Once the RowMatrix is ready we can compute our Singular Value Decomposition\n",
    "\n",
    "\n",
    "# TypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\n",
    "\n",
    "\n",
    "\n",
    "# svd.s\n",
    "# # DenseVector([9.491, 4.6253])\n",
    "# svd.U.rows.collect()\n",
    "# # [DenseVector([0.1129, -0.909]), DenseVector([0.463, 0.4055]), DenseVector([0.8792, -0.0968])]\n",
    "# svd.V\n",
    "# # DenseMatrix(2, 2, [-0.8025, -0.5967, -0.5967, 0.8025], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.87880882005149...|\n",
      "|  0.0|       0.0|[0.84845025303278...|\n",
      "|  0.0|       0.0|[0.80843366665943...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"pca_features\" does not exist.\\nAvailable fields: label, features, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, std_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4990.fit.\n: java.lang.IllegalArgumentException: Field \"pca_features\" does not exist.\nAvailable fields: label, features, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, std_features\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:41)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.LogisticRegression.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:266)\n\tat org.apache.spark.ml.classification.LogisticRegression.validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.GeneratedMethodAccessor130.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-bee8134ce994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrun_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTANDARDIZED_FEATURES_COL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrun_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_FEATURES_COL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-bee8134ce994>\u001b[0m in \u001b[0;36mrun_logistic_regression\u001b[0;34m(feature_vector_col_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_col_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABEL_COL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_vector_col_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mLR_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mLR_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Scikit-Learn uses predict() method for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mLR_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABEL_COL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRINT_ROW_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"pca_features\" does not exist.\\nAvailable fields: label, features, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, std_features'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def run_logistic_regression(feature_vector_col_name: 'str'):\n",
    "    LR = LogisticRegression(labelCol=LABEL_COL_NAME, featuresCol=feature_vector_col_name, maxIter=50)\n",
    "    LR_model = LR.fit(df_train)\n",
    "    LR_preds = LR_model.transform(df_test) # Scikit-Learn uses predict() method for prediction \n",
    "    LR_preds.select(LABEL_COL_NAME, \"prediction\", \"probability\").show(PRINT_ROW_NUM)\n",
    "    #selected.show(PRINT_ROW_NUM)\n",
    "\n",
    "run_logistic_regression(STANDARDIZED_FEATURES_COL_NAME)\n",
    "run_logistic_regression(PCA_FEATURES_COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 0.42929465,  0.1401096 , -0.03364896, -0.43922751, -0.00654812,\n",
      "               0.21920053,  0.10082188,  0.09989184,  0.23796936,  0.20468812,\n",
      "               0.01633952, -0.23483937, -0.24732628,  0.17157324,  0.26937861,\n",
      "               0.30199176,  0.06606606, -0.32354628, -0.0647771 , -0.01753075]])\n",
      "Accuracy: 0.7226277372262774\n",
      "False positive (FP) rate: 0.5149235801028204\n",
      "True positive (TP) rate: 0.7226277372262774\n",
      "Recall: 0.7226277372262774\n",
      "Precision: 0.6976008770664435\n"
     ]
    }
   ],
   "source": [
    "def print_regressor_summary(regressorModel: 'LogisticRegressionModel'):\n",
    "    regressor_summary=regressorModel.summary\n",
    "\n",
    "    print(regressorModel.coefficientMatrix)\n",
    "\n",
    "    # The ratio of both True-True and False-False in all instances\n",
    "    print(\"Accuracy: \" + str(regressor_summary.accuracy))\n",
    "\n",
    "    # Predicted True but Actual False :-(\n",
    "    print(\"False positive (FP) rate: \" + str(regressor_summary.weightedFalsePositiveRate))\n",
    "\n",
    "    # Predicted True with Actual True :-)\n",
    "    print(\"True positive (TP) rate: \" + str(regressor_summary.weightedTruePositiveRate))\n",
    "\n",
    "\n",
    "    print(\"Recall: \" + str(regressor_summary.weightedRecall)) # TP / (TP + FP)\n",
    "    print(\"Precision: \" + str(regressor_summary.weightedPrecision)) # TP / (TP + False Negative (FN))\n",
    "\n",
    "    # print(\"F-measure: \" + str(LR_summary.weightedFMeasure))\n",
    "    \n",
    "print_regressor_summary(regressorModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+--------------------+\n",
      "|label|            features|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|        std_features|        pca_features|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+--------------------+\n",
      "|  0.0|[1.0,1.0,0.0,1.0,...|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|[-0.0010448944788...|[-0.0771260512780...|\n",
      "|  1.0|(20,[0,5,7,13,14,...|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|[1.04384958436457...|[0.62549632311628...|\n",
      "|  0.0|(20,[1,2,4,7,10,1...|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|[-1.0459393733222...|[-0.3399688441772...|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected = predictions.select(LABEL_COL_NAME, \"prediction\", \"probability\")\n",
    "# predictions.collect()[0]([\"prediction\", \"probability\"])\n",
    "predictions.collect()[0][\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_2301edda82e8, numClasses = 2, numFeatures = 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, 21: string, 1_INDEX: double, 1_VEC: vector, 3_INDEX: double, 3_VEC: vector, 4_INDEX: double, 4_VEC: vector, 6_INDEX: double, 6_VEC: vector, 7_INDEX: double, 7_VEC: vector, 9_INDEX: double, 9_VEC: vector, 10_INDEX: double, 10_VEC: vector, 12_INDEX: double, 12_VEC: vector, 14_INDEX: double, 14_VEC: vector, 15_INDEX: double, 15_VEC: vector, 17_INDEX: double, 17_VEC: vector, 19_INDEX: double, 19_VEC: vector, 20_INDEX: double, 20_VEC: vector, label: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ROC'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_2301edda82e8, numClasses = 2, numFeatures = 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, 21: string, 1_INDEX: double, 1_VEC: vector, 3_INDEX: double, 3_VEC: vector, 4_INDEX: double, 4_VEC: vector, 6_INDEX: double, 6_VEC: vector, 7_INDEX: double, 7_VEC: vector, 9_INDEX: double, 9_VEC: vector, 10_INDEX: double, 10_VEC: vector, 12_INDEX: double, 12_VEC: vector, 14_INDEX: double, 14_VEC: vector, 15_INDEX: double, 15_VEC: vector, 17_INDEX: double, 17_VEC: vector, 19_INDEX: double, 19_VEC: vector, 20_INDEX: double, 20_VEC: vector, label: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, 1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, 21: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit model to prepped data\n",
    "lrModel = LogisticRegression().fit(preppedDataDF)\n",
    "\n",
    "# ROC for training data\n",
    "display(lrModel, preppedDataDF, \"ROC\")\n",
    "\n",
    "display(lrModel, preppedDataDF)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [LABEL_COL_NAME, FEATURE_VECTOR_COL_NAME] + df.columns\n",
    "df_1 = preppedDataDF.select(selectedcols)\n",
    "display(df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,6.0,1169.0,4.0,4.0,67.0,2.0,1.0]\n",
      "(20,[0,5,7,13,14,15,16,17,18,19],[2.0,1.0,1.0,48.0,5951.0,2.0,2.0,22.0,1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "#df_1.show(PRINT_ROW_NUM)\n",
    "print(df_1.collect()[0]['features']) # DenseVector\n",
    "print(df_1.collect()[1]['features']) # SparseVEctor WHY????????????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "\n",
    "COL_SUFFIX_IDX = \"_INDEX\"\n",
    "COL_SUFFIX_VEC = \"_VEC\"\n",
    "LABEL_COL_NAME = \"label\"\n",
    "\n",
    "df = df.withColumnRenamed(str(len(df.columns)), LABEL_COL_NAME)\n",
    "\n",
    "# Unforutnately need to loop through all the columns, as there is no 'dtype' attribute for column\n",
    "def get_dtype(df, col_name):\n",
    "    return [dtype for name, dtype in df.dtypes if name == col_name][0]\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # Transform and encode all the non-string columns EXCEPT the last column of target/label \n",
    "    if (get_dtype(df, col_name) == 'string') and (col_name != LABEL_COL_NAME):\n",
    "        # STAGE 1 of 2: TRANSFORMING\n",
    "\n",
    "        # Class StringIndexer (an abstract of Estimator)\n",
    "        stringIndexer = StringIndexer(inputCol=col_name, outputCol=col_name + COL_SUFFIX_IDX)\n",
    "\n",
    "        # Fit a model to the input dataset, i.e., mapping categorical values to indices\n",
    "        model = stringIndexer.fit(df)\n",
    "\n",
    "        # transform converts the words column into feature vectors, adding a new column with those vectors to the DataFrame. \n",
    "        col_indexed_df = model.transform(df)\n",
    "\n",
    "        # STAGE 2 of 2: ENCODING\n",
    "        # Class OneHotEncoder\n",
    "        oneHotEncoder = OneHotEncoder(inputCol=col_name + COL_SUFFIX_IDX, outputCol=col_name + COL_SUFFIX_VEC, dropLast=False)\n",
    "\n",
    "        # NO need to fit()\n",
    "        df = oneHotEncoder.transform(col_indexed_df) # Assgin back to 'df', using the col_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, label: smallint, 1_INDEX: double, 1_VEC: vector, 3_INDEX: double, 3_VEC: vector, 4_INDEX: double, 4_VEC: vector, 6_INDEX: double, 6_VEC: vector, 7_INDEX: double, 7_VEC: vector, 9_INDEX: double, 9_VEC: vector, 10_INDEX: double, 10_VEC: vector, 12_INDEX: double, 12_VEC: vector, 14_INDEX: double, 14_VEC: vector, 15_INDEX: double, 15_VEC: vector, 17_INDEX: double, 17_VEC: vector, 19_INDEX: double, 19_VEC: vector, 20_INDEX: double, 20_VEC: vector]>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema # terser output than that of `df.printSchema()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`dropLast=False`** in OneHotEncoder() ##\n",
    "\n",
    "Let's take a close look at this parameter.\n",
    "\n",
    "As per Spark [doc](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder):\n",
    "\n",
    "\"A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast) because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].\"\n",
    "\n",
    "Linearly dependent? What does that mean? Red Huq has written a superb article on [this](https://inmachineswetrust.com/posts/drop-first-columns/).\n",
    "\n",
    "Comparing the two features column with or without `dropLast=False` via `df_final.collect()[0]['features']`:\n",
    "\n",
    "{with}  \n",
    "`SparseVector(61, {0: 6.0, 1: 1169.0, 2: 4.0, 3: 4.0, 4: 67.0, 5: 2.0, 6: 1.0, 8: 1.0, 12: 1.0, 16: 1.0, 27: 1.0, 32: 1.0, 36: 1.0, 40: 1.0, 44: 1.0, 47: 1.0, 50: 1.0, 53: 1.0, 58: 1.0, 59: 1.0})`\n",
    "\n",
    "{without}  \n",
    "`SparseVector(48, {0: 6.0, 1: 1169.0, 2: 4.0, 3: 4.0, 4: 67.0, 5: 2.0, 6: 1.0, 8: 1.0, 11: 1.0, 14: 1.0, 24: 1.0, 28: 1.0, 31: 1.0, 34: 1.0, 37: 1.0, 39: 1.0, 41: 1.0, 43: 1.0, 47: 1.0})`\n",
    "\n",
    "The difference between 61 and 48? 13! This represent 13 times one (1), an extra category from each column of the 13 that was one-hot-encoded, represented with the prefix \"_VEC\".\n",
    "\n",
    "Why does the {with} scenario has an element of **`12: 1.0`**, while the {wthout} does **`11: 1.0`**? Simply put, the first seven (7) elements i.e., from `0:` to `6:`, are identical because we pick up all the `smallint` datatypes first, no one-hot-encoded columns yet.\n",
    "\n",
    "The difference starts from that point onwards, when we take in the columns with the suffix of \"_VEC\".\n",
    "\n",
    "As for the \"1_VEC\", although both scenarios has one (1) flag in the second one-hot-encoded value column in the `8:`, the {with} scenario occupies between `7:` and `10:` whereas the {wtihout} does between `7:` and `9:`, judging from `1_VEC=SparseVector(4, {1: 1.0})` vs. `1_VEC=SparseVector(3, {1: 1.0})`.\n",
    "\n",
    "(If the value is the first one-hot-encoded value, it will be `7: 1.0` and `1_VEC=SparseVector(4, {**0**: 1.0})` ({with}) and `1_VEC=SparseVector(3, {**0**: 1.0})` ({without}).)\n",
    "\n",
    "Next, the \"3_VEC\" one-hot-encoded columns occupy between `11:` and `15:` {with} and between `10:` and `13:` {without} respectively. Given `3_VEC=SparseVector(5, {1: 1.0})` vs. `3_VEC=SparseVector(4, {1: 1.0})`, the second one-hot-encoded value includes a value of 1, so **`12: 1.0`** ({with}) and **`11: 1.0`** ({without})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the columns with or without `dropLast=False` via `df.collect()[0]`:\n",
    "\n",
    "{with}  \n",
    "1='A11', 2=6, 3='A34', 4='A43', 5=1169, 6='A65', 7='A75', 8=4, 9='A93', 10='A101', 11=4, 12='A121', 13=67, 14='A143', 15='A152', 16=2, 17='A173', 18=1, 19='A192', 20='A201', target=1, 1_INDEX=1.0, 1_VEC=SparseVector(4, {1: 1.0}), 3_INDEX=1.0, 3_VEC=SparseVector(5, {1: 1.0}), 4_INDEX=0.0, 4_VEC=SparseVector(10, {0: 1.0}), 6_INDEX=1.0, 6_VEC=SparseVector(5, {1: 1.0}), 7_INDEX=1.0, 7_VEC=SparseVector(5, {1: 1.0}), 9_INDEX=0.0, 9_VEC=SparseVector(4, {0: 1.0}), 10_INDEX=0.0, 10_VEC=SparseVector(3, {0: 1.0}), 12_INDEX=1.0, 12_VEC=SparseVector(4, {1: 1.0}), 14_INDEX=0.0, 14_VEC=SparseVector(3, {0: 1.0}), 15_INDEX=0.0, 15_VEC=SparseVector(3, {0: 1.0}), 17_INDEX=0.0, 17_VEC=SparseVector(4, {0: 1.0}), 19_INDEX=1.0, **19_VEC=SparseVector(2, {1: 1.0})**, 20_INDEX=0.0, 20_VEC=SparseVector(2, {0: 1.0})\n",
    "\n",
    "{without}  \n",
    "1='A11', 2=6, 3='A34', 4='A43', 5=1169, 6='A65', 7='A75', 8=4, 9='A93', 10='A101', 11=4, 12='A121', 13=67, 14='A143', 15='A152', 16=2, 17='A173', 18=1, 19='A192', 20='A201', target=1, 1_INDEX=1.0, 1_VEC=SparseVector(3, {1: 1.0}), 3_INDEX=1.0, 3_VEC=SparseVector(4, {1: 1.0}), 4_INDEX=0.0, 4_VEC=SparseVector(9, {0: 1.0}), 6_INDEX=1.0, 6_VEC=SparseVector(4, {1: 1.0}), 7_INDEX=1.0, 7_VEC=SparseVector(4, {1: 1.0}), 9_INDEX=0.0, 9_VEC=SparseVector(3, {0: 1.0}), 10_INDEX=0.0, 10_VEC=SparseVector(2, {0: 1.0}), 12_INDEX=1.0, 12_VEC=SparseVector(3, {1: 1.0}), 14_INDEX=0.0, 14_VEC=SparseVector(2, {0: 1.0}), 15_INDEX=0.0, 15_VEC=SparseVector(2, {0: 1.0}), 17_INDEX=0.0, 17_VEC=SparseVector(3, {0: 1.0}), 19_INDEX=1.0, **19_VEC=SparseVector(1, {})**, 20_INDEX=0.0, 20_VEC=SparseVector(1, {0: 1.0})\n",
    "\n",
    "Take a note of the difference between the two 19_VEC columns with bianary outcomes.\n",
    "\n",
    "Also, check out Renjith Madhavan's [Sparse Vector vs. Dense Vector](https://youtu.be/oGwEv82ifrE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "FEATURE_VECTOR_COL_NAME = \"features\"\n",
    "feature_columns = []\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # Transform and encod all the non-string columns EXCEPT the last column of target/label\n",
    "    if ((get_dtype(df, col_name) == 'smallint') or (get_dtype(df, col_name) == 'vector')) \\\n",
    "        and (col_name != LABEL_COL_NAME):\n",
    "\n",
    "        feature_columns.append(col_name)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=feature_columns, outputCol=FEATURE_VECTOR_COL_NAME) # inputCols, NOT inputCol\n",
    "df = vectorAssembler.transform(df)\n",
    "\n",
    "# Spark mandates all the features are bunched up into a single vector for each row, in a single column plus target column\n",
    "df_final = df.select([LABEL_COL_NAME, FEATURE_VECTOR_COL_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(61,[0,1,2,3,4,5,...|\n",
      "|    2|(61,[0,1,2,3,4,5,...|\n",
      "|    1|(61,[0,1,2,3,4,5,...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|        std_features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|(61,[0,1,2,3,4,5,...|[-1.2358594668146...|\n",
      "|    2|(61,[0,1,2,3,4,5,...|[2.24706998404857...|\n",
      "|    1|(61,[0,1,2,3,4,5,...|[-0.7382981166913...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardise the data by scaling the features to have zero mean and unit standard deviation\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "STANDARDIZED_FEATURES_COL_NAME = 'std_features'\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol=FEATURE_VECTOR_COL_NAME, outputCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "standardScaler_model = standardScaler.fit(df_final)\n",
    "df_final_standardized = standardScaler_model.transform(df_final)\n",
    "df_final_standardized.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "PCA_FEATURES_COL_NAME = \"pca_features\"\n",
    "PCA_FEATURES_NUM = 10\n",
    "\n",
    "#df_pca = spark.createDataFrame(features_in_densevector,[RAW_FEATURES])\n",
    "#type(df_pca) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "#df_pca.printSchema()\n",
    "pca = PCA(k=PCA_FEATURES_NUM, inputCol=STANDARDIZED_FEATURES_COL_NAME, outputCol=PCA_FEATURES_COL_NAME)\n",
    "#type(pca_extracted) pyspark.ml.feature.PCA\n",
    "\n",
    "pca_model = pca.fit(df_final_standardized)\n",
    "#type(model) pyspark.ml.feature.PCAModel\n",
    "\n",
    "df_pca = pca_model.transform(df_final_standardized) # this create a DataFrame with the regular features and pca_features\n",
    "#type(features) pyspark.sql.dataframe.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            features|        std_features|        pca_features|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    1|(61,[0,1,2,3,4,5,...|[-1.2358594668146...|[1.33390994130525...|\n",
      "|    2|(61,[0,1,2,3,4,5,...|[2.24706998404857...|[-1.9984449233701...|\n",
      "|    1|(61,[0,1,2,3,4,5,...|[-0.7382981166913...|[-0.4617188222860...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pca.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|[1.33390994130525...|\n",
      "|    2|[-1.9984449233701...|\n",
      "|    1|[-0.4617188222860...|\n",
      "|    1|[0.62925852402674...|\n",
      "|    2|[2.53521350375113...|\n",
      "|    1|[3.78857178936826...|\n",
      "|    1|[0.01376816394437...|\n",
      "|    1|[2.15529559321106...|\n",
      "|    1|[-1.5694664557290...|\n",
      "|    2|[0.48956159931766...|\n",
      "|    2|[-2.4507822931846...|\n",
      "|    2|[-1.1207935641468...|\n",
      "|    1|[-1.5938515316987...|\n",
      "|    2|[0.49717903031187...|\n",
      "|    1|[-1.9282434979345...|\n",
      "|    2|[-2.2902363938718...|\n",
      "|    1|[0.83716137685067...|\n",
      "|    1|[1.23529226678538...|\n",
      "|    2|[4.48195324475500...|\n",
      "|    1|[0.88532493562985...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#feature_columns = df_pca.columns\n",
    "#feature_columns.remove('userId')\n",
    "\n",
    "df_pca = df_pca.drop(FEATURE_VECTOR_COL_NAME, STANDARDIZED_FEATURES_COL_NAME)\n",
    "df_pca = df_pca.withColumnRenamed(PCA_FEATURES_COL_NAME, FEATURE_VECTOR_COL_NAME)\n",
    "df_pca.show()\n",
    "#vector_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'features')\n",
    "#user_movie_ratings_features_df = vector_assembler.transform(user_movie_ratings_df).select(['userId', 'features'])\n",
    "#user_movie_ratings_features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two columns MUST be marked as 'label' and 'features'!\n",
    "# Otherwise, \"Py4JJavaError: An error occurred while calling o1175.fit. : java.lang.IllegalArgumentException: Field \"label\" does not exist.\"\n",
    "\n",
    "df_train, df_test = df_pca.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 0.08901828,  2.04211385, -0.17360403,  0.90765064,  1.37276438,\n",
      "               0.5782314 ,  1.30911028, -0.0947821 ,  0.77794618, -0.83321724],\n",
      "             [-0.08151051, -0.76711332,  0.0805868 , -0.33983091, -0.51693128,\n",
      "              -0.25589781, -0.52957943,  0.01674849, -0.26312979,  0.35464646],\n",
      "             [-0.00750777, -1.27500053,  0.09301723, -0.56781973, -0.8558331 ,\n",
      "              -0.32233359, -0.77953085,  0.07803362, -0.51481639,  0.47857079]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR_model = LR.fit(df_train)\n",
    "print(LR_model.coefficientMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7426108374384236\n",
      "False positive (FP) rate: 0.45430896148680194\n",
      "True positive (TP) rate: 0.7426108374384236\n",
      "Recall: 0.7426108374384236\n",
      "Precision: 0.7256564811246483\n"
     ]
    }
   ],
   "source": [
    "LR_summary=LR_model.summary\n",
    "\n",
    "# The ratio of both True-True and False-False in all instances\n",
    "print(\"Accuracy: \" + str(LR_summary.accuracy))\n",
    "\n",
    "# Predicted True but Actual False :-(\n",
    "print(\"False positive (FP) rate: \" + str(LR_summary.weightedFalsePositiveRate))\n",
    "\n",
    "# Predicted True with Actual True :-)\n",
    "print(\"True positive (TP) rate: \" + str(LR_summary.weightedTruePositiveRate))\n",
    "\n",
    "\n",
    "print(\"Recall: \" + str(LR_summary.weightedRecall)) # TP / (TP + FP)\n",
    "print(\"Precision: \" + str(LR_summary.weightedPrecision)) # TP / (TP + False Negative (FN))\n",
    "\n",
    "# print(\"F-measure: \" + str(LR_summary.weightedFMeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1169.0 [ 0  1  2  3  4  5  6  8 12 16 27 32 36 40 44 47 50 53 58 59] 20\n",
      "58\n",
      "('2', '5', '8', '11', '13', '16', '18', '1_VEC', '3_VEC', '4_VEC', '6_VEC', '7_VEC', '9_VEC', '10_VEC', '12_VEC', '14_VEC', '15_VEC', '17_VEC', '19_VEC', '20_VEC')\n",
      "(6, 1169, 4, 4, 67, 2, 1, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "### SAMPLE OPERATION ###\n",
    "# t = df_final.collect()[0]['features'] # Specify a partiuclar row-and-column intersection\n",
    "# print(t[0], t.indices, , t.indices[18], t.indices.shape[0])\n",
    "\n",
    "# col_names = []\n",
    "# row_values = []\n",
    "\n",
    "# for col_name in feature_columns:\n",
    "#     col_names.append(col_name)\n",
    "\n",
    "#     if get_dtype(df, col_name) == 'smallint':\n",
    "#         row_values.append(df.collect()[0][col_name])\n",
    "#     elif get_dtype(df, col_name) == 'vector' :\n",
    "#         row_values.append(df.collect()[0][col_name][1])    \n",
    "    \n",
    "# print(tuple(col_names))\n",
    "# print(tuple(row_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4b9fd27f2a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfeatures_in_densevector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Note of `, )`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfeastures_for_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_for_densevector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# TypeError: 'float' object is not subscriptable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36msparse\u001b[0;34m(size, *args)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \"\"\"\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size, *args)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0;34m\"\"\" A list of indices corresponding to active entries. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0;34m\"\"\" A list of indices corresponding to active entries. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "#df.select(\"*\").toPandas().values.tolist()\n",
    "\n",
    "features_for_densevector = []\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    if ((get_dtype(df, col_name) == 'smallint') or (get_dtype(df, col_name) == 'double')) \\\n",
    "        and (col_name != LABEL_COL_NAME): \n",
    "\n",
    "        features_for_densevector.append(col_name)\n",
    "\n",
    "        \n",
    "len(features_for_densevector)\n",
    "        \n",
    "features_in_densevector = []\n",
    "feastures_for_ml = []\n",
    "        \n",
    "for idx in df.select(features_for_densevector).toPandas().values.tolist():\n",
    "    features_in_densevector.append((Vectors.dense(idx), )) # Note of `, )`\n",
    "\n",
    "    feastures_for_ml.append(Vectors.sparse(len(features_for_densevector), list(map(int, idx))))\n",
    "    # TypeError: 'float' object is not subscriptable\n",
    "    \n",
    "#features_in_densevector[:3]\n",
    "#features_for_ml[:3]\n",
    "\n",
    "\n",
    "\n",
    "# Vectors.dense([1, 2, 3])\n",
    "# DenseVector([1.0, 2.0, 3.0])\n",
    "\n",
    "# Vectors.dense(1.0, 2.0)\n",
    "# DenseVector([1.0, 2.0])\n",
    "\n",
    "# # convert Sparsevector to Densevector\n",
    "# frequencyDenseVectors = frequencyVectors.map(lambda vector: DenseVector(vector.toArray())\n",
    "                                             \n",
    "# class pyspark.ml.linalg.SparseMatrix(numRows, numCols, colPtrs, rowIndices, values, isTransposed=False)[source]\n",
    "\n",
    "#     Sparse Matrix stored in CSC format.\n",
    "\n",
    "#     toArray()[source]\n",
    "\n",
    "#         Return a numpy.ndarray\n",
    "\n",
    "#     toDense()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-577987b71566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfrequencyDenseVectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_in_densevector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfrequencyDenseVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "frequencyDenseVectors = features_in_densevector.map(lambda vector: SparseVector(vector.toArray()))\n",
    "frequencyDenseVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA vs. SVD ##\n",
    "\n",
    "Andre Perunicic goes over SVD on his succinct [article](https://intoli.com/blog/pca-and-svd/).\n",
    "\n",
    "For someone like me who is penchant for LOTS of visuals, Cory Maklin has done [it](https://towardsdatascience.com/singular-value-decomposition-example-in-python-dab2507d85a0) - with numPy. \n",
    "\n",
    "[Nick walsh](https://stackoverflow.com/a/56012609) also attests that \"SVD is the more stable solution for preserving data integrity due to rounding inaccuracies as a result of computing the product of your dataset by its tranpose matrix (X*XâŠ¤)\" as well as J.M.'s [LÃ¤uchli matrix example](https://math.stackexchange.com/a/359428) and Amoeba's [Eigenvalue Decomposition comparison](https://stats.stackexchange.com/a/87536).\n",
    "\n",
    "Bear in mind that SVD runs much SLOWER as it is computationally more INTENSIVE.\n",
    "\n",
    "Also, take note of Vaquar Khan's [mllib vs. ml](https://stackoverflow.com/a/58799652) and Mohamed Ali Jamaoui's [pySpark vs. Scipy](https://stackoverflow.com/a/58799652) comparions.\n",
    "\n",
    "In reference to [Elias Abou Haydar and Sergul Aydore's discussion](https://stackoverflow.com/a/38107324), let's import their SVD method as Java wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "\n",
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")\n",
    "    \n",
    "def computeSVD(row_matrix, k, computeU=False, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a4c189fab5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#df.values.tolist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vectors' is not defined"
     ]
    }
   ],
   "source": [
    "data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),), (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "data\n",
    "#df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.PCA'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.sql import sqlContext\n",
    "#from pyspark.mllib.utils import MLUtils\n",
    "#from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "RAW_FEATURES = \"raw_feasures\"\n",
    "PCA_FEATURES = \"pca_features\"\n",
    "PCA_FEATURES_NUM = 10\n",
    "\n",
    "#df_pca = spark.createDataFrame(features_in_densevector,[RAW_FEATURES])\n",
    "#type(df_pca) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "#df_pca.printSchema()\n",
    "pca_extracted = PCA(k=PCA_FEATURES_NUM, inputCol=RAW_FEATURES, outputCol=PCA_FEATURES)\n",
    "#type(pca_extracted) pyspark.ml.feature.PCA\n",
    "\n",
    "model = pca_extracted.fit(df_pca)\n",
    "#type(model) pyspark.ml.feature.PCAModel\n",
    "\n",
    "features = model.transform(df_pca) # this create a DataFrame with the regular features and pca_features\n",
    "#type(features) pyspark.sql.dataframe.DataFrame\n",
    "features.collect()[0][PCA_FEATURES]\n",
    "\n",
    "# # We can now extract the pca_features to prepare our RowMatrix.\n",
    "pca_features = features.select(PCA_FEATURES).rdd.map(lambda row : row[0])\n",
    "#type(pca_features) # pyspark.rdd.PipelinedRDD\n",
    "\n",
    "\n",
    "#mat = RowMatrix(pca_features)\n",
    "#type(mat) # pyspark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "#svd_model = RowMatrix(pca_features).computeSVD(PCA_FEATURES_NUM, True)\n",
    "#svd_model = mat.computeSVD(PCA_FEATURES_NUM, True)\n",
    "# svd_model.U.rows.collect()\n",
    "# # [DenseVector([-0.7071, 0.7071]), DenseVector([-0.7071, -0.7071])]\n",
    "\n",
    "# svd_model.s\n",
    "# # DenseVector([3.4641, 3.1623])\n",
    "\n",
    "# svd_model.V\n",
    "# # DenseMatrix(3, 2, [-0.4082, -0.8165, -0.4082, 0.8944, -0.4472, 0.0], 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Once the RowMatrix is ready we can compute our Singular Value Decomposition\n",
    "\n",
    "\n",
    "# TypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\n",
    "\n",
    "\n",
    "\n",
    "# svd.s\n",
    "# # DenseVector([9.491, 4.6253])\n",
    "# svd.U.rows.collect()\n",
    "# # [DenseVector([0.1129, -0.909]), DenseVector([0.463, 0.4055]), DenseVector([0.8792, -0.0968])]\n",
    "# svd.V\n",
    "# # DenseMatrix(2, 2, [-0.8025, -0.5967, -0.5967, 0.8025], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = RowMatrix(pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        raw_feasures|        pca_features|\n",
      "+--------------------+--------------------+\n",
      "|[6.0,1169.0,4.0,4...|[-1169.0204471024...|\n",
      "|[48.0,5951.0,2.0,...|[-5951.1095244771...|\n",
      "|[12.0,2096.0,2.0,...|[-2096.0315357355...|\n",
      "|[42.0,7882.0,2.0,...|[-7882.0900453892...|\n",
      "|[24.0,4870.0,3.0,...|[-4870.0537625318...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#features.collect()[0][PCA_FEATURES]\n",
    "features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e44fabc4ba86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET_COL_NAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df[LABEL_COL_NAME].collect().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'USING column `pca_features` cannot be resolved on the left side of the join. The left-side columns: [target];'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1676.join.\n: org.apache.spark.sql.AnalysisException: USING column `pca_features` cannot be resolved on the left side of the join. The left-side columns: [target];\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$99$$anonfun$apply$71.apply(Analyzer.scala:2347)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$99$$anonfun$apply$71.apply(Analyzer.scala:2347)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$99.apply(Analyzer.scala:2346)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$99.apply(Analyzer.scala:2345)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$commonNaturalJoinProcessing(Analyzer.scala:2345)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$34.applyOrElse(Analyzer.scala:2235)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$34.applyOrElse(Analyzer.scala:2232)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2232)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2231)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.join(Dataset.scala:944)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-dc3a7635dbe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf_PCA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdf_PCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_PCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPCA_FEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be basestring\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'USING column `pca_features` cannot be resolved on the left side of the join. The left-side columns: [target];'"
     ]
    }
   ],
   "source": [
    "#df_pca = features.withColumn(TARGET_COL_NAME, df[TARGET_COL_NAME])\n",
    "#  [raw_feasures#2657, pca_features#2661, target#971 AS target#2689]\n",
    "\n",
    "#features.collect()[971][RAW_FEATURES]\n",
    "#df_final = df_final.withColumn(PCA_FEATURES, features[PCA_FEATURES])\n",
    "\n",
    "\n",
    "df_target = df_final.select(LABEL_COL_NAME)\n",
    "df_target.cache()\n",
    "\n",
    "df_PCA = features.select(PCA_FEATURES)\n",
    "df_PCA.cache()\n",
    "\n",
    "df_target.join(df_PCA, [PCA_FEATURES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input dataset must be a DataFrame but got <class 'pyspark.rdd.PipelinedRDD'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-87ed7404813e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# type(df_pca.collect()[0][RAW_FEATURES])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpca_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvertVectorColumnsToML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCA_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPCA_FEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/mllib/util.py\u001b[0m in \u001b[0;36mconvertVectorColumnsToML\u001b[0;34m(dataset, *cols)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input dataset must be a DataFrame but got {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"convertVectorColumnsToML\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input dataset must be a DataFrame but got <class 'pyspark.rdd.PipelinedRDD'>."
     ]
    }
   ],
   "source": [
    "# TypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\n",
    "#svd = mat.computeSVD(PCA_FEATURES_NUM,True)\n",
    "\n",
    "# from pyspark.mllib.utils import MLUtils\n",
    "from pyspark.mllib.util import MLUtils\n",
    "# df_pca = MLUtils.convertVectorColumnsFromML(df_pca, RAW_FEATURES)\n",
    "#df_pca = MLUtils.convertVectorColumnsToML(df_pca, RAW_FEATURES)\n",
    "# df_pca.collect()[0][RAW_FEATURES]\n",
    "# type(df_pca.collect()[0][RAW_FEATURES])\n",
    "\n",
    "pca_features = MLUtils.convertVectorColumnsToML(pca_features, PCA_FEATURES)\n",
    "type(pca_features.collect()[0][PCA_FEATURES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1211.computeSVD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 98.0 failed 1 times, most recent failure: Lost task 0.0 in stage 98.0 (TID 131, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1382)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1422)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:232)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:208)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-89966fecb3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_FEATURES_NUM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36mcomputeSVD\u001b[0;34m(self, k, computeU, rCond)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m    343\u001b[0m         j_model = self._java_matrix_wrapper.call(\n\u001b[0;32m--> 344\u001b[0;31m             \"computeSVD\", int(k), bool(computeU), float(rCond))\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSingularValueDecomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1211.computeSVD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 98.0 failed 1 times, most recent failure: Lost task 0.0 in stage 98.0 (TID 131, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1382)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1422)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:232)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:208)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\n",
    "svd = mat.computeSVD(PCA_FEATURES_NUM,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_test = df_final.randomSplit(0.80, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0: 0\n",
      "_c1: 0\n",
      "_c2: 0\n",
      "_c3: 0\n",
      "_c4: 0\n",
      "_c5: 0\n",
      "_c6: 0\n",
      "_c7: 0\n",
      "_c8: 0\n",
      "_c9: 0\n",
      "_c10: 0\n",
      "_c11: 0\n",
      "_c12: 0\n",
      "_c13: 0\n",
      "_c14: 0\n",
      "_c15: 0\n",
      "_c16: 0\n",
      "_c17: 0\n",
      "_c18: 0\n",
      "_c19: 0\n",
      "_c20: 0\n"
     ]
    }
   ],
   "source": [
    "#df[\"_c0\"].isNull\n",
    "\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "\n",
    "names = df.schema.names\n",
    "\n",
    "for name in names:\n",
    "    trim(df[name])\n",
    "    \n",
    "    print(name + ': ' + str(df.where(df[name].isNull()).count())) # + ' & ' + str(df.where(df[name] == 'NoneType').count())\n",
    "\n",
    "\n",
    "#duh = df.filter(df[\"_c0\"] == ' ').collect()\n",
    "#duh = df.filter(df[\"_c0\"].isNull()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import column\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "for name in names:\n",
    "    suffix = 'A' + str(int(name.replace('_c', '')) + 1)\n",
    "    \n",
    "#    print(suffix + \": \" + name)\n",
    "    # translate(name, 'A' + str(int(name.replace('_c', '')) + 1), '')\n",
    "\n",
    "    #df.name = df.select(regexp_replace(column(name), '^' + suffix, '')) #.alias(name)\n",
    "    #df.name = df.withColumn(name, regexp_replace(name, '^' + suffix, ''))\n",
    "    \n",
    "    \n",
    "    #targetDf = df.withColumn(name, \\\n",
    "    #          when(df[\"session\"] == 0, 999).otherwise(df[\"timestamp1\"]))\n",
    "    \n",
    "# use VectorAssembler for One hot encoding or indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 0, not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6c3b33d0b65c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(10,10), marker = 'o', hist_kwds = {'bins': 10}, s = 60, alpha = 0.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, alpha=.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pair_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_misc.py\u001b[0m in \u001b[0;36mscatter_matrix\u001b[0;34m(frame, alpha, figsize, ax, grid, diagonal, marker, density_kwds, hist_kwds, range_padding, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mhist_kwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist_kwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mrange_padding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange_padding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_matplotlib/misc.py\u001b[0m in \u001b[0;36mscatter_matrix\u001b[0;34m(frame, alpha, figsize, ax, grid, diagonal, marker, density_kwds, hist_kwds, range_padding, **kwds)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnaxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnaxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# no gaps between subplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py\u001b[0m in \u001b[0;36m_subplots\u001b[0;34m(naxes, sharex, sharey, squeeze, subplot_kw, ax, layout, layout_type, **fig_kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# Create first subplot separately, so we can share it if requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0max0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 59\u001b[0;31m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[1;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[1;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 0, not 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_pair_plot = df.select(\"*\").toPandas()\n",
    "\n",
    "#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(10,10), marker = 'o', hist_kwds = {'bins': 10}, s = 60, alpha = 0.8)\n",
    "#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, alpha=.8)\n",
    "g = pd.plotting.scatter_matrix(df_pair_plot, 0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1| 2053|\n",
      "|      0| 4313|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|rate_marriage|count|\n",
      "+-------------+-----+\n",
      "|            1|   99|\n",
      "|            3|  993|\n",
      "|            5| 2684|\n",
      "|            4| 2242|\n",
      "|            2|  348|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('rate_marriage').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+\n",
      "|rate_marriage|affairs|count|\n",
      "+-------------+-------+-----+\n",
      "|            1|      0|   25|\n",
      "|            1|      1|   74|\n",
      "|            2|      0|  127|\n",
      "|            2|      1|  221|\n",
      "|            3|      0|  446|\n",
      "|            3|      1|  547|\n",
      "|            4|      0| 1518|\n",
      "|            4|      1|  724|\n",
      "|            5|      0| 2197|\n",
      "|            5|      1|  487|\n",
      "+-------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('rate_marriage','affairs').count().orderBy('rate_marriage','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|religious|affairs|count|\n",
      "+---------+-------+-----+\n",
      "|        1|      0|  613|\n",
      "|        1|      1|  408|\n",
      "|        2|      0| 1448|\n",
      "|        2|      1|  819|\n",
      "|        3|      0| 1715|\n",
      "|        3|      1|  707|\n",
      "|        4|      0|  537|\n",
      "|        4|      1|  119|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('religious','affairs').count().orderBy('religious','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "|children|affairs|count|\n",
      "+--------+-------+-----+\n",
      "|     0.0|      0| 1912|\n",
      "|     0.0|      1|  502|\n",
      "|     1.0|      0|  747|\n",
      "|     1.0|      1|  412|\n",
      "|     2.0|      0|  873|\n",
      "|     2.0|      1|  608|\n",
      "|     3.0|      0|  460|\n",
      "|     3.0|      1|  321|\n",
      "|     4.0|      0|  197|\n",
      "|     4.0|      1|  131|\n",
      "|     5.5|      0|  124|\n",
      "|     5.5|      1|   79|\n",
      "+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('children','affairs').count().orderBy('children','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "|affairs|avg(rate_marriage)|          avg(age)|  avg(yrs_married)|     avg(children)|    avg(religious)|avg(affairs)|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "|      1|3.6473453482708234|30.537018996590355|11.152459814905017|1.7289332683877252| 2.261568436434486|         1.0|\n",
      "|      0| 4.329700904242986| 28.39067934152562| 7.989334569904939|1.2388128912589844|2.5045212149316023|         0.0|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('affairs').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['rate_marriage', 'age', 'yrs_married', 'children', 'religious'], outputCol=\"features\")\n",
    "df = df_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rate_marriage: integer (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- yrs_married: double (nullable = true)\n",
      " |-- children: double (nullable = true)\n",
      " |-- religious: integer (nullable = true)\n",
      " |-- affairs: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------+\n",
      "|features               |affairs|\n",
      "+-----------------------+-------+\n",
      "|[5.0,32.0,6.0,1.0,3.0] |0      |\n",
      "|[4.0,22.0,2.5,0.0,2.0] |0      |\n",
      "|[3.0,32.0,9.0,3.0,3.0] |1      |\n",
      "|[3.0,27.0,13.0,3.0,1.0]|1      |\n",
      "|[4.0,22.0,2.5,0.0,1.0] |1      |\n",
      "|[4.0,37.0,16.5,4.0,3.0]|1      |\n",
      "|[5.0,27.0,9.0,1.0,1.0] |1      |\n",
      "|[4.0,27.0,9.0,0.0,2.0] |1      |\n",
      "|[5.0,37.0,23.0,5.5,2.0]|1      |\n",
      "|[5.0,37.0,23.0,5.5,2.0]|1      |\n",
      "+-----------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['features','affairs']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data for building model\n",
    "model_df=df.select(['features','affairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=model_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1| 1574|\n",
      "|      0| 3226|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1|  479|\n",
      "|      0| 1087|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier=RandomForestClassifier(labelCol='affairs',numTrees=50).fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions=rf_classifier.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|affairs|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.0,22.0,2.5,1.0...|      1|[18.7524598757082...|[0.37504919751416...|       1.0|\n",
      "|[1.0,27.0,2.5,0.0...|      1|[21.0021478623554...|[0.42004295724710...|       1.0|\n",
      "|[1.0,27.0,6.0,0.0...|      0|[20.1166611727778...|[0.40233322345555...|       1.0|\n",
      "|[1.0,27.0,6.0,2.0...|      1|[19.0905206218080...|[0.38181041243616...|       1.0|\n",
      "|[1.0,27.0,6.0,3.0...|      0|[16.3348579592130...|[0.32669715918426...|       1.0|\n",
      "|[1.0,27.0,9.0,4.0...|      0|[13.3128973003485...|[0.26625794600697...|       1.0|\n",
      "|[1.0,32.0,13.0,0....|      1|[16.7812008990910...|[0.33562401798182...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[12.6966189294366...|[0.25393237858873...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[12.6766379097881...|[0.25353275819576...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[16.5966383870776...|[0.33193276774155...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[16.5966383870776...|[0.33193276774155...|       1.0|\n",
      "|[1.0,32.0,16.5,3....|      1|[18.0962335004510...|[0.36192467000902...|       1.0|\n",
      "|[1.0,37.0,16.5,1....|      1|[17.1671412215484...|[0.34334282443096...|       1.0|\n",
      "|[1.0,37.0,16.5,2....|      1|[16.9033235593205...|[0.33806647118641...|       1.0|\n",
      "|[1.0,37.0,23.0,4....|      1|[12.9328949963349...|[0.25865789992669...|       1.0|\n",
      "|[1.0,37.0,23.0,5....|      1|[19.2882486885513...|[0.38576497377102...|       1.0|\n",
      "|[1.0,42.0,16.5,5....|      1|[14.7895576971207...|[0.29579115394241...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[13.1159769754090...|[0.26231953950818...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[16.8901830088537...|[0.33780366017707...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[16.8901830088537...|[0.33780366017707...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 1261|\n",
      "|       1.0|  305|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-------+----------+\n",
      "|probability                             |affairs|prediction|\n",
      "+----------------------------------------+-------+----------+\n",
      "|[0.37504919751416455,0.6249508024858356]|1      |1.0       |\n",
      "|[0.42004295724710805,0.579957042752892] |1      |1.0       |\n",
      "|[0.40233322345555694,0.597666776544443] |0      |1.0       |\n",
      "|[0.3818104124361619,0.6181895875638381] |1      |1.0       |\n",
      "|[0.32669715918426007,0.6733028408157399]|0      |1.0       |\n",
      "|[0.26625794600697006,0.7337420539930299]|0      |1.0       |\n",
      "|[0.3356240179818214,0.6643759820181787] |1      |1.0       |\n",
      "|[0.25393237858873335,0.7460676214112667]|1      |1.0       |\n",
      "|[0.2535327581957624,0.7464672418042376] |1      |1.0       |\n",
      "|[0.3319327677415531,0.6680672322584469] |1      |1.0       |\n",
      "+----------------------------------------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.select(['probability','affairs','prediction']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_accuracy=MulticlassClassificationEvaluator(labelCol='affairs',metricName='accuracy').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of RF on test data is 73%\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of RF on test data is {0:.0%}'.format(rf_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7279693486590039\n"
     ]
    }
   ],
   "source": [
    "print(rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_precision=MulticlassClassificationEvaluator(labelCol='affairs',metricName='weightedPrecision').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision rate on test data is 71%\n"
     ]
    }
   ],
   "source": [
    "print('The precision rate on test data is {0:.0%}'.format(rf_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7085017563673452"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_auc=BinaryClassificationEvaluator(labelCol='affairs').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421702296835062\n"
     ]
    }
   ],
   "source": [
    "print(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.5536, 1: 0.0364, 2: 0.2358, 3: 0.0803, 4: 0.0939})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': [{'idx': 0, 'name': 'rate_marriage'},\n",
       "  {'idx': 1, 'name': 'age'},\n",
       "  {'idx': 2, 'name': 'yrs_married'},\n",
       "  {'idx': 3, 'name': 'children'},\n",
       "  {'idx': 4, 'name': 'religious'}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.save(\"/home/jovyan/work/RF_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassificationModel.load(\"/home/jovyan/work/RF_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preditions=rf.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|affairs|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.0,22.0,2.5,1.0...|      1|[188.676639360932...|[0.37735327872186...|       1.0|\n",
      "|[1.0,27.0,2.5,0.0...|      1|[195.425833792250...|[0.39085166758450...|       1.0|\n",
      "|[1.0,27.0,6.0,0.0...|      0|[193.138478579040...|[0.38627695715808...|       1.0|\n",
      "|[1.0,27.0,6.0,2.0...|      1|[185.424877645536...|[0.37084975529107...|       1.0|\n",
      "|[1.0,27.0,6.0,3.0...|      0|[164.685852316351...|[0.32937170463270...|       1.0|\n",
      "|[1.0,27.0,9.0,4.0...|      0|[142.006095001922...|[0.28401219000384...|       1.0|\n",
      "|[1.0,32.0,13.0,0....|      1|[176.885399312490...|[0.35377079862498...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[128.585405941664...|[0.25717081188332...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[126.963464206019...|[0.25392692841203...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[153.429839787005...|[0.30685967957401...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[153.429839787005...|[0.30685967957401...|       1.0|\n",
      "|[1.0,32.0,16.5,3....|      1|[167.493344562280...|[0.33498668912456...|       1.0|\n",
      "|[1.0,37.0,16.5,1....|      1|[150.090425556233...|[0.30018085111246...|       1.0|\n",
      "|[1.0,37.0,16.5,2....|      1|[154.326506925977...|[0.30865301385195...|       1.0|\n",
      "|[1.0,37.0,23.0,4....|      1|[123.559481897677...|[0.24711896379535...|       1.0|\n",
      "|[1.0,37.0,23.0,5....|      1|[192.317134975430...|[0.38463426995086...|       1.0|\n",
      "|[1.0,42.0,16.5,5....|      1|[151.350101320899...|[0.30270020264179...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[126.937590310819...|[0.25387518062163...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[156.647804216314...|[0.31329560843262...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[156.647804216314...|[0.31329560843262...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_preditions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
