{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'local[*]' - instructing to use all the cores (*) on a local machine\n",
    "# 'appName' - will be given a random name unless specified, like Docker container image name\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder. \\\n",
    "        master('local[*]').\\\n",
    "        appName('credit risk').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source CSV file must reside in the same folder\n",
    "# The 'german.data-numeric' file delimits columns with 2 spaces ('  ')\n",
    "#   Without data scrubbing, \"IllegalArgumentException: 'Delimiter cannot be more than one character:   '\"\n",
    "df = spark.read.csv('german.data',sep=' ',inferSchema=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 21)\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      "\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_c0|_c1|_c2|_c3| _c4|_c5|_c6|_c7|_c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|   4|A121|  67|A143|A152|   2|A173|   1|A192|A201|   1|\n",
      "|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|   2|A121|  22|A143|A152|   1|A173|   1|A191|A201|   2|\n",
      "|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|   3|A121|  49|A143|A152|   1|A172|   2|A191|A201|   1|\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_ROW_NUM = 3\n",
    "COL_TOTAL_NUM = len(df.columns)\n",
    "\n",
    "print((df.count(),COL_TOTAL_NUM))\n",
    "df.printSchema()\n",
    "df.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the columns with numbers are \"**ordinal**\" i.e., the magnitude in each column/feature means \"something\" (e.g., age, duration, credit amount and installment rate). Thus, **transformation** and **encoding** is NOT needed and they will be converted to non-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ShortType # a signed 16-bit integer vs. IntegerType (a signed 32-bit integer)\n",
    "\n",
    "col_indices_of_ordinal_numbers = [2, 5, 8, 11, 13, 16, 18] # Ignore the target/label column i.e., '_c20', the last column\n",
    "col_indices_of_categorical_numbers = []\n",
    "\n",
    "def convert_integer_to_string_in_list(aList: list) -> list:\n",
    "    return [str(item) for item in aList]\n",
    "\n",
    "for col_idx in range(1, COL_TOTAL_NUM):\n",
    "    if col_idx not in col_indices_of_ordinal_numbers:\n",
    "        col_indices_of_categorical_numbers.append(col_idx)\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # change the name of each column e.g., \"_c0\" -> \"1\"\n",
    "    new_col_num = int(col_name.replace('_c', '')) + 1\n",
    "    df = df.withColumnRenamed(col_name, str(new_col_num))\n",
    "    \n",
    "    if new_col_num in col_indices_of_ordinal_numbers:  \n",
    "        df = df.withColumn(str(new_col_num), df[str(new_col_num)].cast(ShortType()))\n",
    "        \n",
    "col_indices_of_ordinal_numbers = convert_integer_to_string_in_list(col_indices_of_ordinal_numbers)\n",
    "col_indices_of_categorical_numbers = convert_integer_to_string_in_list(col_indices_of_categorical_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|\n",
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|\n",
      "|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|\n",
      "|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|\n",
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'string'),\n",
       " ('2', 'smallint'),\n",
       " ('3', 'string'),\n",
       " ('4', 'string'),\n",
       " ('5', 'smallint'),\n",
       " ('6', 'string'),\n",
       " ('7', 'string'),\n",
       " ('8', 'smallint'),\n",
       " ('9', 'string'),\n",
       " ('10', 'string'),\n",
       " ('11', 'smallint'),\n",
       " ('12', 'string'),\n",
       " ('13', 'smallint'),\n",
       " ('14', 'string'),\n",
       " ('15', 'string'),\n",
       " ('16', 'smallint'),\n",
       " ('17', 'string'),\n",
       " ('18', 'smallint'),\n",
       " ('19', 'string'),\n",
       " ('20', 'string'),\n",
       " ('21', 'string')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(PRINT_ROW_NUM)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Databricks**, the leading crowd platform for massive scale data engineering and collaborative data science with optimized Apache Sparkâ„¢ clusters on AWS or Azure, just pubslihed articles [here](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5703650563260092/2738738457173388/3024994340111770/latest.html) and [there](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7274796508260238/2844662950865680/2879908466733289/latest.html) on Apache Spark MLlib Pipelines API. Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "COL_SUFFIX_IDX = \"_INDEX\"\n",
    "COL_SUFFIX_VEC = \"_VEC\"\n",
    "\n",
    "stages = []\n",
    "\n",
    "for col_idx in col_indices_of_categorical_numbers:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer = StringIndexer(inputCol=col_idx, outputCol=col_idx + COL_SUFFIX_IDX)\n",
    "    \n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[col_idx + COL_SUFFIX_VEC])\n",
    "    \n",
    "    # Add stages.  These are not run here, but will run all at once later on.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_COL_NAME = \"label\"\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol=str(COL_TOTAL_NUM), outputCol=LABEL_COL_NAME)\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all features into a vector using VectorAssembler\n",
    "FEATURE_VECTOR_COL_NAME = \"features\"\n",
    "\n",
    "assemblerInputs = [col_idx + COL_SUFFIX_IDX for col_idx in col_indices_of_categorical_numbers] + col_indices_of_ordinal_numbers\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=FEATURE_VECTOR_COL_NAME)\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "STANDARDIZED_FEATURES_COL_NAME = 'std_features'\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol=FEATURE_VECTOR_COL_NAME, outputCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "stages += [standardScaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(df)\n",
    "df_pipeline = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics, MulticlassMetrics\n",
    "\n",
    "def print_performance_metrics(predictions):\n",
    "  # Evaluate model\n",
    "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "  auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "  aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "  print(\"auc = {}\".format(auc))\n",
    "  print(\"aupr = {}\".format(aupr))\n",
    "\n",
    "  # get rdd of predictions and labels for mllib eval metrics\n",
    "  predictionAndLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "\n",
    "  # Instantiate metrics objects\n",
    "  binary_metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "  multi_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "  # Area under precision-recall curve\n",
    "  print(\"Area under PR = {}\".format(binary_metrics.areaUnderPR))\n",
    "  # Area under ROC curve\n",
    "  print(\"Area under ROC = {}\".format(binary_metrics.areaUnderROC))\n",
    "  # Accuracy\n",
    "  print(\"Accuracy = {}\".format(multi_metrics.accuracy))\n",
    "  # Confusion Matrix\n",
    "  print(multi_metrics.confusionMatrix())\n",
    "  \n",
    "  # F1\n",
    "  print(\"F1 = {}\".format(multi_metrics.fMeasure()))\n",
    "  # Precision\n",
    "  print(\"Precision = {}\".format(multi_metrics.precision()))\n",
    "  # Recall\n",
    "  print(\"Recall = {}\".format(multi_metrics.recall()))\n",
    "  # FPR\n",
    "  print(\"FPR = {}\".format(multi_metrics.falsePositiveRate(0.0)))\n",
    "  # TPR\n",
    "  print(\"TPR = {}\".format(multi_metrics.truePositiveRate(0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 0.11832168009876139\n",
      "2: 0.20081984440853262\n",
      "3: 0.2759429720647131\n",
      "4: 0.3434691583369005\n",
      "5: 0.4068048276404337\n",
      "6: 0.4646192825677727\n",
      "7: 0.5212579515868058\n",
      "8: 0.5726833221543236\n",
      "9: 0.620595299353551\n",
      "10: 0.66634309561251\n",
      "11: 0.71070821099644\n",
      "12: 0.7534198111210798\n",
      "13: 0.7946167029948367\n",
      "14: 0.8334120028543393\n",
      "15: 0.8697509575845473\n",
      "16: 0.9040447458263459\n",
      "17: 0.936440526700285\n",
      "18: 0.9657619466572893\n",
      "19: 0.9871407977706779\n",
      "20: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "PCA_FEATURES_COL_NAME = \"pca_features\"\n",
    "MAX_ITER_NUM = 50\n",
    "\n",
    "\n",
    "PCA_feature_num_list = np.arange(1, COL_TOTAL_NUM, 1)\n",
    "PCA_explainedVariances_list = []\n",
    "#cumulative_explained_variance = 0\n",
    "\n",
    "for PCA_feature_num in PCA_feature_num_list:\n",
    "    PCA_extracted = PCA(k=PCA_feature_num, inputCol=STANDARDIZED_FEATURES_COL_NAME, outputCol=PCA_FEATURES_COL_NAME)\n",
    "\n",
    "    pca_model = PCA_extracted.fit(df_pipeline)\n",
    "    df_PCA = pca_model.transform(df_pipeline)\n",
    "\n",
    "    df_train, df_test = df_PCA.randomSplit([0.7, 0.3], seed = 100)\n",
    "    \n",
    "    LR = LogisticRegression(labelCol=LABEL_COL_NAME, featuresCol=PCA_FEATURES_COL_NAME, maxIter=MAX_ITER_NUM)\n",
    "    LR_model = LR.fit(df_train)\n",
    "    LR_preds = LR_model.transform(df_test) # Scikit-Learn uses predict() method for prediction\n",
    "\n",
    "#     print(pca_model.explainedVariance) [0.11832168009876139,0.08249816430977125,0.07512312765618046]\n",
    "#     print(np.cumsum(pca_model.explainedVariance)) [0.11832168 0.20081984 0.27594297]\n",
    "\n",
    "    cumulative_explained_variance = np.cumsum(pca_model.explainedVariance)[PCA_feature_num - 1]\n",
    "\n",
    "    PCA_explainedVariances_list.append(cumulative_explained_variance)\n",
    "    print(str(PCA_feature_num) + \": \" + str(cumulative_explained_variance))\n",
    "\n",
    "#     LR_preds.select(LABEL_COL_NAME, \"prediction\", \"rawPrediction\", \"probability\").show(PRINT_ROW_NUM)\n",
    "#     print_performance_metrics(LR_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "ax.plot(PCA_feature_num_list, PCA_explainedVariances_list)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty apparent that ALL the features should be used in making prediction, as each component noteciably adds variance in relation to the total variance - almost in a straight-line fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITER_NUM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "PARAMETER_CHOICE_NUM = 3\n",
    "classification_model_and_paramgrid_list = []\n",
    "\n",
    "LR = LogisticRegression(labelCol=LABEL_COL_NAME, featuresCol=STANDARDIZED_FEATURES_COL_NAME) # , maxIter=MAX_ITER_NUM)\n",
    "LR_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(LR.regParam, np.linspace(0.01, 2, PARAMETER_CHOICE_NUM))\n",
    "             .addGrid(LR.elasticNetParam, np.linspace(0, 1, PARAMETER_CHOICE_NUM))\n",
    "             .addGrid(LR.maxIter, np.linspace(1, MAX_ITER_NUM, PARAMETER_CHOICE_NUM, dtype = int))\n",
    "             .build())\n",
    "classification_model_and_paramgrid_list.append([LR, LR_paramGrid])\n",
    "\n",
    "\n",
    "RF = RandomForestClassifier(labelCol=LABEL_COL_NAME, featuresCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "RF_ParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(RF.numTrees, np.linspace(1, MAX_ITER_NUM, PARAMETER_CHOICE_NUM, dtype = int))\n",
    "             .addGrid(RF.maxDepth, np.linspace(1, COL_TOTAL_NUM, PARAMETER_CHOICE_NUM))\n",
    "             .build())\n",
    "classification_model_and_paramgrid_list.append([RF, RF_ParamGrid])\n",
    "\n",
    "\n",
    "GB = GBTClassifier(labelCol=LABEL_COL_NAME, featuresCol=STANDARDIZED_FEATURES_COL_NAME) #, maxIter=MAX_ITER_NUM)\n",
    "GB_ParamGrid = (ParamGridBuilder()\n",
    "             .addGrid(GB.maxIter, np.linspace(1, MAX_ITER_NUM, PARAMETER_CHOICE_NUM, dtype = int))\n",
    "             .addGrid(GB.maxDepth, np.linspace(1, COL_TOTAL_NUM, PARAMETER_CHOICE_NUM))\n",
    "             .build())\n",
    "classification_model_and_paramgrid_list.append([GB, GB_ParamGrid])\n",
    "\n",
    "\n",
    "LSVC = LinearSVC(maxIter=MAX_ITER_NUM)\n",
    "LSVC_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(LSVC.regParam, np.linspace(0.01, 2, PARAMETER_CHOICE_NUM))\n",
    "             .addGrid(LSVC.maxIter, np.linspace(1, MAX_ITER_NUM, PARAMETER_CHOICE_NUM, dtype = int))\n",
    "             .build())\n",
    "classification_model_and_paramgrid_list.append([LSVC, LSVC_paramGrid])\n",
    "\n",
    "\n",
    "# OVR = OneVsRest(classifier=LR)\n",
    "# OVR_paramGrid = (ParamGridBuilder().build())\n",
    "# classification_model_and_paramgrid_list.append([OVR, OVR_paramGrid])\n",
    "# IllegalArgumentException: 'Field \"rawPrediction\" does not exist.\\nAvailable fields: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 1_INDEX, 1_VEC, 3_INDEX, 3_VEC, 4_INDEX, 4_VEC, 6_INDEX, 6_VEC, 7_INDEX, 7_VEC, 9_INDEX, 9_VEC, 10_INDEX, 10_VEC, 12_INDEX, 12_VEC, 14_INDEX, 14_VEC, 15_INDEX, 15_VEC, 17_INDEX, 17_VEC, 19_INDEX, 19_VEC, 20_INDEX, 20_VEC, label, features, std_features, pca_features, CrossValidator_01aa8f348031_rand, prediction'    \n",
    "\n",
    "NB = NaiveBayes()\n",
    "NB_paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(NB.smoothing, np.linspace(0, 1, PARAMETER_CHOICE_NUM))\n",
    "             .build())\n",
    "classification_model_and_paramgrid_list.append([NB, NB_paramGrid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7225377876191389\n",
      "0.7742851641474918\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "for classification_model in classification_model_and_paramgrid_list:\n",
    "#     # Train model with Training Data\n",
    "#     model = classification_model.fit(df_train)\n",
    "\n",
    "#     # Make predictions on test data using the Transformer.transform() method.\n",
    "#     predictions = model.transform(df_test)\n",
    "\n",
    "#     # Evaluate model\n",
    "#     evaluator = BinaryClassificationEvaluator()\n",
    "#     evaluator.evaluate(predictions)\n",
    "#     # evaluator.getMetricName() # By default, areaUnderROC\n",
    "\n",
    "    # Create 5-fold CrossValidator\n",
    "    cv = CrossValidator(estimator=classification_model[0], \n",
    "                        estimatorParamMaps=classification_model[1],\n",
    "                        evaluator=evaluator,\n",
    "                        numFolds=5)\n",
    "\n",
    "    # Run cross validations\n",
    "    cvModel = cv.fit(df_train)\n",
    "    # this will likely take a fair amount of time because of the amount of models that we're creating and testing\n",
    "\n",
    "    # Use test set to measure the accuracy of our model on new data\n",
    "    predictions = cvModel.transform(df_test)\n",
    "\n",
    "    # cvModel uses the best model found from the Cross Validation\n",
    "    # Evaluate best model\n",
    "    \n",
    "    print(evaluator.evaluate(predictions))\n",
    "    \n",
    "    model_details_in_list = classification_model_and_paramgrid_list[classification_model_and_paramgrid_list.index(classification_model)]\n",
    "    \n",
    "    model_details_in_list.append(evaluator.evaluate(predictions))\n",
    "    model_details_in_list.append(cvModel.bestModel)\n",
    "    \n",
    "    print(classification_model_and_paramgrid_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model_and_paramgrid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "|label|            features|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|        std_features|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "|  0.0|[1.0,1.0,0.0,1.0,...|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|[-0.0010448944788...|\n",
      "|  1.0|(20,[0,5,7,13,14,...|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|[1.04384958436457...|\n",
      "|  0.0|(20,[1,2,4,7,10,1...|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|[-1.0459393733222...|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardise the data by scaling the features to have zero mean and unit standard deviation\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "STANDARDIZED_FEATURES_COL_NAME = 'std_features'\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol=FEATURE_VECTOR_COL_NAME, outputCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "standardScaler_model = standardScaler.fit(df_pipeline)\n",
    "df_final_standardized = standardScaler_model.transform(df_pipeline)\n",
    "df_final_standardized.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "\n",
    "#RAW_FEATURES = \"raw_feasures\"\n",
    "PCA_FEATURES_COL_NAME = \"pca_features\"\n",
    "PCA_FEATURES_NUM = int(COL_TOTAL_NUM / 2)\n",
    "\n",
    "#df_pca = spark.createDataFrame(features_in_densevector,[RAW_FEATURES])\n",
    "#type(df_pca) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "#df_pca.printSchema()\n",
    "PCA_extracted = PCA(k=PCA_FEATURES_NUM, inputCol=STANDARDIZED_FEATURES_COL_NAME, outputCol=PCA_FEATURES_COL_NAME)\n",
    "\n",
    "stages += [PCA_extracted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the stages as a Pipeline.\n",
    "# This puts the data through all of the feature transformations we described in a single call.\n",
    "  \n",
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(df)\n",
    "df_pipeline = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "315\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df_pipeline.randomSplit([0.7, 0.3], seed = 100)\n",
    "print (df_train.count())\n",
    "print (df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, 1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, 21: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### IS THIS NECESSARY?????\n",
    "\n",
    "# # Keep relevant columns \n",
    "# # label column is the transformation of Income\n",
    "# # Features are the numberical representation of all the data numerical+categorical. \n",
    "# selected_cols = [LABEL_COL_NAME, FEATURE_VECTOR_COL_NAME] + df.columns\n",
    "# df_pipeline = df_pipeline.select(selected_cols)\n",
    "# df_pipeline.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "|label|            features|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|        std_features|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "|  0.0|[1.0,1.0,0.0,1.0,...|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|[-0.0010448944788...|\n",
      "|  1.0|(20,[0,5,7,13,14,...|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|[1.04384958436457...|\n",
      "|  0.0|(20,[1,2,4,7,10,1...|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|[-1.0459393733222...|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardise the data by scaling the features to have zero mean and unit standard deviation\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "STANDARDIZED_FEATURES_COL_NAME = 'std_features'\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol=FEATURE_VECTOR_COL_NAME, outputCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "standardScaler_model = standardScaler.fit(df_pipeline)\n",
    "df_final_standardized = standardScaler_model.transform(df_pipeline)\n",
    "df_final_standardized.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685\n",
      "315\n"
     ]
    }
   ],
   "source": [
    "# Randomly split data into training and test sets. set seed for reproducibility\n",
    "# Simialr to sklearn test_train_split\n",
    "df_train, df_test = df_final_standardized.randomSplit([0.7, 0.3], seed = 100)\n",
    "print (df_train.count())\n",
    "print (df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** STANDARDIZED FEATURES ***\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|label|prediction|       rawPrediction|         probability|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|  1.0|       0.0|[1.98878882577249...|[0.87961494222793...|\n",
      "|  0.0|       0.0|[2.59363122276489...|[0.93045056953822...|\n",
      "|  0.0|       0.0|[2.65212943642400...|[0.93414211593761...|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "DenseMatrix([[ 0.40849242,  0.15921003,  0.10444606, -0.35825602,  0.10119409,\n",
      "               0.32339495,  0.13529649,  0.16577387,  0.199668  ,  0.16229824,\n",
      "              -0.0841887 , -0.22727041, -0.25552495,  0.27869821,  0.20885488,\n",
      "               0.36299363,  0.01800106, -0.23550216, -0.04776179,  0.10293513]])\n",
      "Accuracy: 0.7313868613138687\n",
      "False positive (FP) rate: 0.5056232356373986\n",
      "True positive (TP) rate: 0.7313868613138687\n",
      "Recall: 0.7313868613138687\n",
      "Precision: 0.7093861144728328\n",
      "\n",
      "\n",
      "*** PCA-SELECTED FEATURES ***\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|label|prediction|       rawPrediction|         probability|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "|  1.0|       0.0|[1.78128211581802...|[0.85585510904920...|\n",
      "|  0.0|       0.0|[1.75759183138588...|[0.85290779747608...|\n",
      "|  0.0|       0.0|[2.83314045213547...|[0.94444061974445...|\n",
      "+-----+----------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "DenseMatrix([[-0.20532344,  0.34605834, -0.09899167, -0.00437557,  0.06953392,\n",
      "               0.65440333, -0.28637784, -0.09768004,  0.11710577, -0.00641603]])\n",
      "Accuracy: 0.7182481751824817\n",
      "False positive (FP) rate: 0.5389405561541663\n",
      "True positive (TP) rate: 0.7182481751824817\n",
      "Recall: 0.7182481751824817\n",
      "Precision: 0.6902125843941309\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def run_logistic_regression(feature_vector_col_name: 'str'):\n",
    "    def print_regressor_summary(regressorModel: 'LogisticRegressionModel'):\n",
    "        regressor_summary=regressorModel.summary\n",
    "\n",
    "        print(regressorModel.coefficientMatrix)\n",
    "\n",
    "        # The ratio of both True-True and False-False in all instances\n",
    "        print(\"Accuracy: \" + str(regressor_summary.accuracy))\n",
    "\n",
    "        # Predicted True but Actual False :-(\n",
    "        print(\"False positive (FP) rate: \" + str(regressor_summary.weightedFalsePositiveRate))\n",
    "\n",
    "        # Predicted True with Actual True :-)\n",
    "        print(\"True positive (TP) rate: \" + str(regressor_summary.weightedTruePositiveRate))\n",
    "\n",
    "\n",
    "        print(\"Recall: \" + str(regressor_summary.weightedRecall)) # TP / (TP + FP)\n",
    "        print(\"Precision: \" + str(regressor_summary.weightedPrecision)) # TP / (TP + False Negative (FN))\n",
    "\n",
    "        # print(\"F-measure: \" + str(LR_summary.weightedFMeasure))    \n",
    "\n",
    "    LR = LogisticRegression(labelCol=LABEL_COL_NAME, featuresCol=feature_vector_col_name, maxIter=50)\n",
    "    LR_model = LR.fit(df_train)\n",
    "    LR_preds = LR_model.transform(df_test) # Scikit-Learn uses predict() method for prediction\n",
    "    LR_preds.select(LABEL_COL_NAME, \"prediction\", \"rawPrediction\", \"probability\").show(PRINT_ROW_NUM)\n",
    "    \n",
    "    print_regressor_summary(LR_model)\n",
    "\n",
    "print(\"*** STANDARDIZED FEATURES ***\")\n",
    "run_logistic_regression(STANDARDIZED_FEATURES_COL_NAME)\n",
    "print(\"\\n\\n*** PCA-SELECTED FEATURES ***\")\n",
    "run_logistic_regression(PCA_FEATURES_COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 0.42929465,  0.1401096 , -0.03364896, -0.43922751, -0.00654812,\n",
      "               0.21920053,  0.10082188,  0.09989184,  0.23796936,  0.20468812,\n",
      "               0.01633952, -0.23483937, -0.24732628,  0.17157324,  0.26937861,\n",
      "               0.30199176,  0.06606606, -0.32354628, -0.0647771 , -0.01753075]])\n",
      "Accuracy: 0.7226277372262774\n",
      "False positive (FP) rate: 0.5149235801028204\n",
      "True positive (TP) rate: 0.7226277372262774\n",
      "Recall: 0.7226277372262774\n",
      "Precision: 0.6976008770664435\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def print_performance_metrics(predictions):\n",
    "  # Evaluate model\n",
    "  evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "  auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "  aupr = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderPR\"})\n",
    "  print(\"auc = {}\".format(auc))\n",
    "  print(\"aupr = {}\".format(aupr))\n",
    "\n",
    "  # get rdd of predictions and labels for mllib eval metrics\n",
    "  predictionAndLabels = predictions.select(\"prediction\",\"label\").rdd\n",
    "\n",
    "  # Instantiate metrics objects\n",
    "  binary_metrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "  multi_metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "  # Area under precision-recall curve\n",
    "  print(\"Area under PR = {}\".format(binary_metrics.areaUnderPR))\n",
    "  # Area under ROC curve\n",
    "  print(\"Area under ROC = {}\".format(binary_metrics.areaUnderROC))\n",
    "  # Accuracy\n",
    "  print(\"Accuracy = {}\".format(multi_metrics.accuracy))\n",
    "  # Confusion Matrix\n",
    "  print(multi_metrics.confusionMatrix())\n",
    "  \n",
    "  ### Question 5.1 Answer ###\n",
    "  \n",
    "  # F1\n",
    "  print(\"F1 = {}\".format(multi_metrics.fMeasure()))\n",
    "  # Precision\n",
    "  print(\"Precision = {}\".format(multi_metrics.precision()))\n",
    "  # Recall\n",
    "  print(\"Recall = {}\".format(multi_metrics.recall()))\n",
    "  # FPR\n",
    "  print(\"FPR = {}\".format(multi_metrics.falsePositiveRate(0.0)))\n",
    "  # TPR\n",
    "  print(\"TPR = {}\".format(multi_metrics.truePositiveRate(0.0)))\n",
    "  \n",
    "  \n",
    "print_performance_metrics(lrPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.sql import sqlContext\n",
    "#from pyspark.mllib.utils import MLUtils\n",
    "#from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "#RAW_FEATURES = \"raw_feasures\"\n",
    "PCA_FEATURES_COL_NAME = \"pca_features\"\n",
    "PCA_FEATURES_NUM = int(COL_TOTAL_NUM / 2)\n",
    "\n",
    "#df_pca = spark.createDataFrame(features_in_densevector,[RAW_FEATURES])\n",
    "#type(df_pca) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "#df_pca.printSchema()\n",
    "PCA_extracted = PCA(k=PCA_FEATURES_NUM, inputCol=STANDARDIZED_FEATURES_COL_NAME, outputCol=PCA_FEATURES_COL_NAME)\n",
    "#type(pca_extracted) pyspark.ml.feature.PCA\n",
    "\n",
    "PCA_model = pca_extracted.fit(df_final_standardized)\n",
    "#type(model) pyspark.ml.feature.PCAModel\n",
    "\n",
    "df_PCA = PCA_model.transform(df_final_standardized) # this create a DataFrame with the regular features and pca_features\n",
    "#type(features) pyspark.sql.dataframe.DataFrame\n",
    "features.collect()[0][PCA_FEATURES]\n",
    "\n",
    "# # We can now extract the pca_features to prepare our RowMatrix.\n",
    "pca_features = df_PCA.select(PCA_FEATURES).rdd.map(lambda row : row[0])\n",
    "#type(pca_features) # pyspark.rdd.PipelinedRDD\n",
    "\n",
    "\n",
    "#mat = RowMatrix(pca_features)\n",
    "#type(mat) # pyspark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "#svd_model = RowMatrix(pca_features).computeSVD(PCA_FEATURES_NUM, True)\n",
    "#svd_model = mat.computeSVD(PCA_FEATURES_NUM, True)\n",
    "# svd_model.U.rows.collect()\n",
    "# # [DenseVector([-0.7071, 0.7071]), DenseVector([-0.7071, -0.7071])]\n",
    "\n",
    "# svd_model.s\n",
    "# # DenseVector([3.4641, 3.1623])\n",
    "\n",
    "# svd_model.V\n",
    "# # DenseMatrix(3, 2, [-0.4082, -0.8165, -0.4082, 0.8944, -0.4472, 0.0], 0)\n",
    "\n",
    "\n",
    "# # Once the RowMatrix is ready we can compute our Singular Value Decomposition\n",
    "\n",
    "\n",
    "# TypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\n",
    "\n",
    "\n",
    "\n",
    "# svd.s\n",
    "# # DenseVector([9.491, 4.6253])\n",
    "# svd.U.rows.collect()\n",
    "# # [DenseVector([0.1129, -0.909]), DenseVector([0.463, 0.4055]), DenseVector([0.8792, -0.0968])]\n",
    "# svd.V\n",
    "# # DenseMatrix(2, 2, [-0.8025, -0.5967, -0.5967, 0.8025], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.87880882005149...|\n",
      "|  0.0|       0.0|[0.84845025303278...|\n",
      "|  0.0|       0.0|[0.80843366665943...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"pca_features\" does not exist.\\nAvailable fields: label, features, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, std_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4990.fit.\n: java.lang.IllegalArgumentException: Field \"pca_features\" does not exist.\nAvailable fields: label, features, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, std_features\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n\tat org.apache.spark.ml.util.SchemaUtils$.checkColumnType(SchemaUtils.scala:41)\n\tat org.apache.spark.ml.PredictorParams$class.validateAndTransformSchema(Predictor.scala:51)\n\tat org.apache.spark.ml.classification.Classifier.org$apache$spark$ml$classification$ClassifierParams$$super$validateAndTransformSchema(Classifier.scala:58)\n\tat org.apache.spark.ml.classification.ClassifierParams$class.validateAndTransformSchema(Classifier.scala:42)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifier.org$apache$spark$ml$classification$ProbabilisticClassifierParams$$super$validateAndTransformSchema(ProbabilisticClassifier.scala:53)\n\tat org.apache.spark.ml.classification.ProbabilisticClassifierParams$class.validateAndTransformSchema(ProbabilisticClassifier.scala:37)\n\tat org.apache.spark.ml.classification.LogisticRegression.org$apache$spark$ml$classification$LogisticRegressionParams$$super$validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.classification.LogisticRegressionParams$class.validateAndTransformSchema(LogisticRegression.scala:266)\n\tat org.apache.spark.ml.classification.LogisticRegression.validateAndTransformSchema(LogisticRegression.scala:279)\n\tat org.apache.spark.ml.Predictor.transformSchema(Predictor.scala:144)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:100)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n\tat sun.reflect.GeneratedMethodAccessor130.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-bee8134ce994>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mrun_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTANDARDIZED_FEATURES_COL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mrun_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_FEATURES_COL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-bee8134ce994>\u001b[0m in \u001b[0;36mrun_logistic_regression\u001b[0;34m(feature_vector_col_name)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_vector_col_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mLR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABEL_COL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_vector_col_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxIter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mLR_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mLR_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Scikit-Learn uses predict() method for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mLR_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABEL_COL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"probability\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPRINT_ROW_NUM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: 'Field \"pca_features\" does not exist.\\nAvailable fields: label, features, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, std_features'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "def run_logistic_regression(feature_vector_col_name: 'str'):\n",
    "    LR = LogisticRegression(labelCol=LABEL_COL_NAME, featuresCol=feature_vector_col_name, maxIter=50)\n",
    "    LR_model = LR.fit(df_train)\n",
    "    LR_preds = LR_model.transform(df_test) # Scikit-Learn uses predict() method for prediction \n",
    "    LR_preds.select(LABEL_COL_NAME, \"prediction\", \"probability\").show(PRINT_ROW_NUM)\n",
    "    #selected.show(PRINT_ROW_NUM)\n",
    "\n",
    "run_logistic_regression(STANDARDIZED_FEATURES_COL_NAME)\n",
    "run_logistic_regression(PCA_FEATURES_COL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 0.42929465,  0.1401096 , -0.03364896, -0.43922751, -0.00654812,\n",
      "               0.21920053,  0.10082188,  0.09989184,  0.23796936,  0.20468812,\n",
      "               0.01633952, -0.23483937, -0.24732628,  0.17157324,  0.26937861,\n",
      "               0.30199176,  0.06606606, -0.32354628, -0.0647771 , -0.01753075]])\n",
      "Accuracy: 0.7226277372262774\n",
      "False positive (FP) rate: 0.5149235801028204\n",
      "True positive (TP) rate: 0.7226277372262774\n",
      "Recall: 0.7226277372262774\n",
      "Precision: 0.6976008770664435\n"
     ]
    }
   ],
   "source": [
    "def print_regressor_summary(regressorModel: 'LogisticRegressionModel'):\n",
    "    regressor_summary=regressorModel.summary\n",
    "\n",
    "    print(regressorModel.coefficientMatrix)\n",
    "\n",
    "    # The ratio of both True-True and False-False in all instances\n",
    "    print(\"Accuracy: \" + str(regressor_summary.accuracy))\n",
    "\n",
    "    # Predicted True but Actual False :-(\n",
    "    print(\"False positive (FP) rate: \" + str(regressor_summary.weightedFalsePositiveRate))\n",
    "\n",
    "    # Predicted True with Actual True :-)\n",
    "    print(\"True positive (TP) rate: \" + str(regressor_summary.weightedTruePositiveRate))\n",
    "\n",
    "\n",
    "    print(\"Recall: \" + str(regressor_summary.weightedRecall)) # TP / (TP + FP)\n",
    "    print(\"Precision: \" + str(regressor_summary.weightedPrecision)) # TP / (TP + False Negative (FN))\n",
    "\n",
    "    # print(\"F-measure: \" + str(LR_summary.weightedFMeasure))\n",
    "    \n",
    "print_regressor_summary(regressorModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+--------------------+\n",
      "|label|            features|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|        std_features|        pca_features|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+--------------------+\n",
      "|  0.0|[1.0,1.0,0.0,1.0,...|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|[-0.0010448944788...|[-0.0771260512780...|\n",
      "|  1.0|(20,[0,5,7,13,14,...|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|[1.04384958436457...|[0.62549632311628...|\n",
      "|  0.0|(20,[1,2,4,7,10,1...|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|[-1.0459393733222...|[-0.3399688441772...|\n",
      "+-----+--------------------+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selected = predictions.select(LABEL_COL_NAME, \"prediction\", \"probability\")\n",
    "# predictions.collect()[0]([\"prediction\", \"probability\"])\n",
    "predictions.collect()[0][\"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_2301edda82e8, numClasses = 2, numFeatures = 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, 21: string, 1_INDEX: double, 1_VEC: vector, 3_INDEX: double, 3_VEC: vector, 4_INDEX: double, 4_VEC: vector, 6_INDEX: double, 6_VEC: vector, 7_INDEX: double, 7_VEC: vector, 9_INDEX: double, 9_VEC: vector, 10_INDEX: double, 10_VEC: vector, 12_INDEX: double, 12_VEC: vector, 14_INDEX: double, 14_VEC: vector, 15_INDEX: double, 15_VEC: vector, 17_INDEX: double, 17_VEC: vector, 19_INDEX: double, 19_VEC: vector, 20_INDEX: double, 20_VEC: vector, label: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'ROC'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionModel: uid = LogisticRegression_2301edda82e8, numClasses = 2, numFeatures = 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, 21: string, 1_INDEX: double, 1_VEC: vector, 3_INDEX: double, 3_VEC: vector, 4_INDEX: double, 4_VEC: vector, 6_INDEX: double, 6_VEC: vector, 7_INDEX: double, 7_VEC: vector, 9_INDEX: double, 9_VEC: vector, 10_INDEX: double, 10_VEC: vector, 12_INDEX: double, 12_VEC: vector, 14_INDEX: double, 14_VEC: vector, 15_INDEX: double, 15_VEC: vector, 17_INDEX: double, 17_VEC: vector, 19_INDEX: double, 19_VEC: vector, 20_INDEX: double, 20_VEC: vector, label: double, features: vector]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[label: double, features: vector, 1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, 21: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Fit model to prepped data\n",
    "lrModel = LogisticRegression().fit(preppedDataDF)\n",
    "\n",
    "# ROC for training data\n",
    "display(lrModel, preppedDataDF, \"ROC\")\n",
    "\n",
    "display(lrModel, preppedDataDF)\n",
    "\n",
    "# Keep relevant columns\n",
    "selectedcols = [LABEL_COL_NAME, FEATURE_VECTOR_COL_NAME] + df.columns\n",
    "df_1 = preppedDataDF.select(selectedcols)\n",
    "display(df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,1.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,6.0,1169.0,4.0,4.0,67.0,2.0,1.0]\n",
      "(20,[0,5,7,13,14,15,16,17,18,19],[2.0,1.0,1.0,48.0,5951.0,2.0,2.0,22.0,1.0,1.0])\n"
     ]
    }
   ],
   "source": [
    "#df_1.show(PRINT_ROW_NUM)\n",
    "print(df_1.collect()[0]['features']) # DenseVector\n",
    "print(df_1.collect()[1]['features']) # SparseVEctor WHY????????????????????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "\n",
    "COL_SUFFIX_IDX = \"_INDEX\"\n",
    "COL_SUFFIX_VEC = \"_VEC\"\n",
    "LABEL_COL_NAME = \"label\"\n",
    "\n",
    "df = df.withColumnRenamed(str(len(df.columns)), LABEL_COL_NAME)\n",
    "\n",
    "# Unforutnately need to loop through all the columns, as there is no 'dtype' attribute for column\n",
    "def get_dtype(df, col_name):\n",
    "    return [dtype for name, dtype in df.dtypes if name == col_name][0]\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # Transform and encode all the non-string columns EXCEPT the last column of target/label \n",
    "    if (get_dtype(df, col_name) == 'string') and (col_name != LABEL_COL_NAME):\n",
    "        # STAGE 1 of 2: TRANSFORMING\n",
    "\n",
    "        # Class StringIndexer (an abstract of Estimator)\n",
    "        stringIndexer = StringIndexer(inputCol=col_name, outputCol=col_name + COL_SUFFIX_IDX)\n",
    "\n",
    "        # Fit a model to the input dataset, i.e., mapping categorical values to indices\n",
    "        model = stringIndexer.fit(df)\n",
    "\n",
    "        # transform converts the words column into feature vectors, adding a new column with those vectors to the DataFrame. \n",
    "        col_indexed_df = model.transform(df)\n",
    "\n",
    "        # STAGE 2 of 2: ENCODING\n",
    "        # Class OneHotEncoder\n",
    "        oneHotEncoder = OneHotEncoder(inputCol=col_name + COL_SUFFIX_IDX, outputCol=col_name + COL_SUFFIX_VEC, dropLast=False)\n",
    "\n",
    "        # NO need to fit()\n",
    "        df = oneHotEncoder.transform(col_indexed_df) # Assgin back to 'df', using the col_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, label: smallint, 1_INDEX: double, 1_VEC: vector, 3_INDEX: double, 3_VEC: vector, 4_INDEX: double, 4_VEC: vector, 6_INDEX: double, 6_VEC: vector, 7_INDEX: double, 7_VEC: vector, 9_INDEX: double, 9_VEC: vector, 10_INDEX: double, 10_VEC: vector, 12_INDEX: double, 12_VEC: vector, 14_INDEX: double, 14_VEC: vector, 15_INDEX: double, 15_VEC: vector, 17_INDEX: double, 17_VEC: vector, 19_INDEX: double, 19_VEC: vector, 20_INDEX: double, 20_VEC: vector]>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema # terser output than that of `df.printSchema()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`dropLast=False`** in OneHotEncoder() ##\n",
    "\n",
    "Let's take a close look at this parameter.\n",
    "\n",
    "As per Spark [doc](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.OneHotEncoder):\n",
    "\n",
    "\"A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast) because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].\"\n",
    "\n",
    "Linearly dependent? What does that mean? Red Huq has written a superb article on [this](https://inmachineswetrust.com/posts/drop-first-columns/).\n",
    "\n",
    "Comparing the two features column with or without `dropLast=False` via `df_final.collect()[0]['features']`:\n",
    "\n",
    "{with}  \n",
    "`SparseVector(61, {0: 6.0, 1: 1169.0, 2: 4.0, 3: 4.0, 4: 67.0, 5: 2.0, 6: 1.0, 8: 1.0, 12: 1.0, 16: 1.0, 27: 1.0, 32: 1.0, 36: 1.0, 40: 1.0, 44: 1.0, 47: 1.0, 50: 1.0, 53: 1.0, 58: 1.0, 59: 1.0})`\n",
    "\n",
    "{without}  \n",
    "`SparseVector(48, {0: 6.0, 1: 1169.0, 2: 4.0, 3: 4.0, 4: 67.0, 5: 2.0, 6: 1.0, 8: 1.0, 11: 1.0, 14: 1.0, 24: 1.0, 28: 1.0, 31: 1.0, 34: 1.0, 37: 1.0, 39: 1.0, 41: 1.0, 43: 1.0, 47: 1.0})`\n",
    "\n",
    "The difference between 61 and 48? 13! This represent 13 times one (1), an extra category from each column of the 13 that was one-hot-encoded, represented with the prefix \"_VEC\".\n",
    "\n",
    "Why does the {with} scenario has an element of **`12: 1.0`**, while the {wthout} does **`11: 1.0`**? Simply put, the first seven (7) elements i.e., from `0:` to `6:`, are identical because we pick up all the `smallint` datatypes first, no one-hot-encoded columns yet.\n",
    "\n",
    "The difference starts from that point onwards, when we take in the columns with the suffix of \"_VEC\".\n",
    "\n",
    "As for the \"1_VEC\", although both scenarios has one (1) flag in the second one-hot-encoded value column in the `8:`, the {with} scenario occupies between `7:` and `10:` whereas the {wtihout} does between `7:` and `9:`, judging from `1_VEC=SparseVector(4, {1: 1.0})` vs. `1_VEC=SparseVector(3, {1: 1.0})`.\n",
    "\n",
    "(If the value is the first one-hot-encoded value, it will be `7: 1.0` and `1_VEC=SparseVector(4, {**0**: 1.0})` ({with}) and `1_VEC=SparseVector(3, {**0**: 1.0})` ({without}).)\n",
    "\n",
    "Next, the \"3_VEC\" one-hot-encoded columns occupy between `11:` and `15:` {with} and between `10:` and `13:` {without} respectively. Given `3_VEC=SparseVector(5, {1: 1.0})` vs. `3_VEC=SparseVector(4, {1: 1.0})`, the second one-hot-encoded value includes a value of 1, so **`12: 1.0`** ({with}) and **`11: 1.0`** ({without})."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the columns with or without `dropLast=False` via `df.collect()[0]`:\n",
    "\n",
    "{with}  \n",
    "1='A11', 2=6, 3='A34', 4='A43', 5=1169, 6='A65', 7='A75', 8=4, 9='A93', 10='A101', 11=4, 12='A121', 13=67, 14='A143', 15='A152', 16=2, 17='A173', 18=1, 19='A192', 20='A201', target=1, 1_INDEX=1.0, 1_VEC=SparseVector(4, {1: 1.0}), 3_INDEX=1.0, 3_VEC=SparseVector(5, {1: 1.0}), 4_INDEX=0.0, 4_VEC=SparseVector(10, {0: 1.0}), 6_INDEX=1.0, 6_VEC=SparseVector(5, {1: 1.0}), 7_INDEX=1.0, 7_VEC=SparseVector(5, {1: 1.0}), 9_INDEX=0.0, 9_VEC=SparseVector(4, {0: 1.0}), 10_INDEX=0.0, 10_VEC=SparseVector(3, {0: 1.0}), 12_INDEX=1.0, 12_VEC=SparseVector(4, {1: 1.0}), 14_INDEX=0.0, 14_VEC=SparseVector(3, {0: 1.0}), 15_INDEX=0.0, 15_VEC=SparseVector(3, {0: 1.0}), 17_INDEX=0.0, 17_VEC=SparseVector(4, {0: 1.0}), 19_INDEX=1.0, **19_VEC=SparseVector(2, {1: 1.0})**, 20_INDEX=0.0, 20_VEC=SparseVector(2, {0: 1.0})\n",
    "\n",
    "{without}  \n",
    "1='A11', 2=6, 3='A34', 4='A43', 5=1169, 6='A65', 7='A75', 8=4, 9='A93', 10='A101', 11=4, 12='A121', 13=67, 14='A143', 15='A152', 16=2, 17='A173', 18=1, 19='A192', 20='A201', target=1, 1_INDEX=1.0, 1_VEC=SparseVector(3, {1: 1.0}), 3_INDEX=1.0, 3_VEC=SparseVector(4, {1: 1.0}), 4_INDEX=0.0, 4_VEC=SparseVector(9, {0: 1.0}), 6_INDEX=1.0, 6_VEC=SparseVector(4, {1: 1.0}), 7_INDEX=1.0, 7_VEC=SparseVector(4, {1: 1.0}), 9_INDEX=0.0, 9_VEC=SparseVector(3, {0: 1.0}), 10_INDEX=0.0, 10_VEC=SparseVector(2, {0: 1.0}), 12_INDEX=1.0, 12_VEC=SparseVector(3, {1: 1.0}), 14_INDEX=0.0, 14_VEC=SparseVector(2, {0: 1.0}), 15_INDEX=0.0, 15_VEC=SparseVector(2, {0: 1.0}), 17_INDEX=0.0, 17_VEC=SparseVector(3, {0: 1.0}), 19_INDEX=1.0, **19_VEC=SparseVector(1, {})**, 20_INDEX=0.0, 20_VEC=SparseVector(1, {0: 1.0})\n",
    "\n",
    "Take a note of the difference between the two 19_VEC columns with bianary outcomes.\n",
    "\n",
    "Also, check out Renjith Madhavan's [Sparse Vector vs. Dense Vector](https://youtu.be/oGwEv82ifrE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "FEATURE_VECTOR_COL_NAME = \"features\"\n",
    "feature_columns = []\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # Transform and encod all the non-string columns EXCEPT the last column of target/label\n",
    "    if ((get_dtype(df, col_name) == 'smallint') or (get_dtype(df, col_name) == 'vector')) \\\n",
    "        and (col_name != LABEL_COL_NAME):\n",
    "\n",
    "        feature_columns.append(col_name)\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=feature_columns, outputCol=FEATURE_VECTOR_COL_NAME) # inputCols, NOT inputCol\n",
    "df = vectorAssembler.transform(df)\n",
    "\n",
    "# Spark mandates all the features are bunched up into a single vector for each row, in a single column plus target column\n",
    "df_final = df.select([LABEL_COL_NAME, FEATURE_VECTOR_COL_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|(61,[0,1,2,3,4,5,...|\n",
      "|    2|(61,[0,1,2,3,4,5,...|\n",
      "|    1|(61,[0,1,2,3,4,5,...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|        std_features|\n",
      "+-----+--------------------+--------------------+\n",
      "|    1|(61,[0,1,2,3,4,5,...|[-1.2358594668146...|\n",
      "|    2|(61,[0,1,2,3,4,5,...|[2.24706998404857...|\n",
      "|    1|(61,[0,1,2,3,4,5,...|[-0.7382981166913...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standardise the data by scaling the features to have zero mean and unit standard deviation\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "STANDARDIZED_FEATURES_COL_NAME = 'std_features'\n",
    "\n",
    "standardScaler = StandardScaler(withMean=True, withStd=True, inputCol=FEATURE_VECTOR_COL_NAME, outputCol=STANDARDIZED_FEATURES_COL_NAME)\n",
    "standardScaler_model = standardScaler.fit(df_final)\n",
    "df_final_standardized = standardScaler_model.transform(df_final)\n",
    "df_final_standardized.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "PCA_FEATURES_COL_NAME = \"pca_features\"\n",
    "PCA_FEATURES_NUM = 10\n",
    "\n",
    "#df_pca = spark.createDataFrame(features_in_densevector,[RAW_FEATURES])\n",
    "#type(df_pca) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "#df_pca.printSchema()\n",
    "pca = PCA(k=PCA_FEATURES_NUM, inputCol=STANDARDIZED_FEATURES_COL_NAME, outputCol=PCA_FEATURES_COL_NAME)\n",
    "#type(pca_extracted) pyspark.ml.feature.PCA\n",
    "\n",
    "pca_model = pca.fit(df_final_standardized)\n",
    "#type(model) pyspark.ml.feature.PCAModel\n",
    "\n",
    "df_pca = pca_model.transform(df_final_standardized) # this create a DataFrame with the regular features and pca_features\n",
    "#type(features) pyspark.sql.dataframe.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            features|        std_features|        pca_features|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|    1|(61,[0,1,2,3,4,5,...|[-1.2358594668146...|[1.33390994130525...|\n",
      "|    2|(61,[0,1,2,3,4,5,...|[2.24706998404857...|[-1.9984449233701...|\n",
      "|    1|(61,[0,1,2,3,4,5,...|[-0.7382981166913...|[-0.4617188222860...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pca.show(PRINT_ROW_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    1|[1.33390994130525...|\n",
      "|    2|[-1.9984449233701...|\n",
      "|    1|[-0.4617188222860...|\n",
      "|    1|[0.62925852402674...|\n",
      "|    2|[2.53521350375113...|\n",
      "|    1|[3.78857178936826...|\n",
      "|    1|[0.01376816394437...|\n",
      "|    1|[2.15529559321106...|\n",
      "|    1|[-1.5694664557290...|\n",
      "|    2|[0.48956159931766...|\n",
      "|    2|[-2.4507822931846...|\n",
      "|    2|[-1.1207935641468...|\n",
      "|    1|[-1.5938515316987...|\n",
      "|    2|[0.49717903031187...|\n",
      "|    1|[-1.9282434979345...|\n",
      "|    2|[-2.2902363938718...|\n",
      "|    1|[0.83716137685067...|\n",
      "|    1|[1.23529226678538...|\n",
      "|    2|[4.48195324475500...|\n",
      "|    1|[0.88532493562985...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#feature_columns = df_pca.columns\n",
    "#feature_columns.remove('userId')\n",
    "\n",
    "df_pca = df_pca.drop(FEATURE_VECTOR_COL_NAME, STANDARDIZED_FEATURES_COL_NAME)\n",
    "df_pca = df_pca.withColumnRenamed(PCA_FEATURES_COL_NAME, FEATURE_VECTOR_COL_NAME)\n",
    "df_pca.show()\n",
    "#vector_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'features')\n",
    "#user_movie_ratings_features_df = vector_assembler.transform(user_movie_ratings_df).select(['userId', 'features'])\n",
    "#user_movie_ratings_features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two columns MUST be marked as 'label' and 'features'!\n",
    "# Otherwise, \"Py4JJavaError: An error occurred while calling o1175.fit. : java.lang.IllegalArgumentException: Field \"label\" does not exist.\"\n",
    "\n",
    "df_train, df_test = df_pca.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 0.08901828,  2.04211385, -0.17360403,  0.90765064,  1.37276438,\n",
      "               0.5782314 ,  1.30911028, -0.0947821 ,  0.77794618, -0.83321724],\n",
      "             [-0.08151051, -0.76711332,  0.0805868 , -0.33983091, -0.51693128,\n",
      "              -0.25589781, -0.52957943,  0.01674849, -0.26312979,  0.35464646],\n",
      "             [-0.00750777, -1.27500053,  0.09301723, -0.56781973, -0.8558331 ,\n",
      "              -0.32233359, -0.77953085,  0.07803362, -0.51481639,  0.47857079]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR_model = LR.fit(df_train)\n",
    "print(LR_model.coefficientMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7426108374384236\n",
      "False positive (FP) rate: 0.45430896148680194\n",
      "True positive (TP) rate: 0.7426108374384236\n",
      "Recall: 0.7426108374384236\n",
      "Precision: 0.7256564811246483\n"
     ]
    }
   ],
   "source": [
    "LR_summary=LR_model.summary\n",
    "\n",
    "# The ratio of both True-True and False-False in all instances\n",
    "print(\"Accuracy: \" + str(LR_summary.accuracy))\n",
    "\n",
    "# Predicted True but Actual False :-(\n",
    "print(\"False positive (FP) rate: \" + str(LR_summary.weightedFalsePositiveRate))\n",
    "\n",
    "# Predicted True with Actual True :-)\n",
    "print(\"True positive (TP) rate: \" + str(LR_summary.weightedTruePositiveRate))\n",
    "\n",
    "\n",
    "print(\"Recall: \" + str(LR_summary.weightedRecall)) # TP / (TP + FP)\n",
    "print(\"Precision: \" + str(LR_summary.weightedPrecision)) # TP / (TP + False Negative (FN))\n",
    "\n",
    "# print(\"F-measure: \" + str(LR_summary.weightedFMeasure))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1169.0 [ 0  1  2  3  4  5  6  8 12 16 27 32 36 40 44 47 50 53 58 59] 20\n",
      "58\n",
      "('2', '5', '8', '11', '13', '16', '18', '1_VEC', '3_VEC', '4_VEC', '6_VEC', '7_VEC', '9_VEC', '10_VEC', '12_VEC', '14_VEC', '15_VEC', '17_VEC', '19_VEC', '20_VEC')\n",
      "(6, 1169, 4, 4, 67, 2, 1, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "### SAMPLE OPERATION ###\n",
    "# t = df_final.collect()[0]['features'] # Specify a partiuclar row-and-column intersection\n",
    "# print(t[0], t.indices, , t.indices[18], t.indices.shape[0])\n",
    "\n",
    "# col_names = []\n",
    "# row_values = []\n",
    "\n",
    "# for col_name in feature_columns:\n",
    "#     col_names.append(col_name)\n",
    "\n",
    "#     if get_dtype(df, col_name) == 'smallint':\n",
    "#         row_values.append(df.collect()[0][col_name])\n",
    "#     elif get_dtype(df, col_name) == 'vector' :\n",
    "#         row_values.append(df.collect()[0][col_name][1])    \n",
    "    \n",
    "# print(tuple(col_names))\n",
    "# print(tuple(row_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4b9fd27f2a5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfeatures_in_densevector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Note of `, )`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfeastures_for_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_for_densevector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# TypeError: 'float' object is not subscriptable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36msparse\u001b[0;34m(size, *args)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \"\"\"\n\u001b[0;32m--> 782\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, size, *args)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0;34m\"\"\" A list of indices corresponding to active entries. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/ml/linalg/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    503\u001b[0m                 \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0;34m\"\"\" A list of indices corresponding to active entries. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.linalg import SparseVector\n",
    "#df.select(\"*\").toPandas().values.tolist()\n",
    "\n",
    "features_for_densevector = []\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    if ((get_dtype(df, col_name) == 'smallint') or (get_dtype(df, col_name) == 'double')) \\\n",
    "        and (col_name != LABEL_COL_NAME): \n",
    "\n",
    "        features_for_densevector.append(col_name)\n",
    "\n",
    "        \n",
    "len(features_for_densevector)\n",
    "        \n",
    "features_in_densevector = []\n",
    "feastures_for_ml = []\n",
    "        \n",
    "for idx in df.select(features_for_densevector).toPandas().values.tolist():\n",
    "    features_in_densevector.append((Vectors.dense(idx), )) # Note of `, )`\n",
    "\n",
    "    feastures_for_ml.append(Vectors.sparse(len(features_for_densevector), list(map(int, idx))))\n",
    "    # TypeError: 'float' object is not subscriptable\n",
    "    \n",
    "#features_in_densevector[:3]\n",
    "#features_for_ml[:3]\n",
    "\n",
    "\n",
    "\n",
    "# Vectors.dense([1, 2, 3])\n",
    "# DenseVector([1.0, 2.0, 3.0])\n",
    "\n",
    "# Vectors.dense(1.0, 2.0)\n",
    "# DenseVector([1.0, 2.0])\n",
    "\n",
    "# # convert Sparsevector to Densevector\n",
    "# frequencyDenseVectors = frequencyVectors.map(lambda vector: DenseVector(vector.toArray())\n",
    "                                             \n",
    "# class pyspark.ml.linalg.SparseMatrix(numRows, numCols, colPtrs, rowIndices, values, isTransposed=False)[source]\n",
    "\n",
    "#     Sparse Matrix stored in CSC format.\n",
    "\n",
    "#     toArray()[source]\n",
    "\n",
    "#         Return a numpy.ndarray\n",
    "\n",
    "#     toDense()[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-577987b71566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfrequencyDenseVectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_in_densevector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSparseVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfrequencyDenseVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "frequencyDenseVectors = features_in_densevector.map(lambda vector: SparseVector(vector.toArray()))\n",
    "frequencyDenseVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA vs. SVD ##\n",
    "\n",
    "Andre Perunicic goes over SVD on his succinct [article](https://intoli.com/blog/pca-and-svd/).\n",
    "\n",
    "For someone like me who is penchant for LOTS of visuals, Cory Maklin has done [it](https://towardsdatascience.com/singular-value-decomposition-example-in-python-dab2507d85a0) - with numPy. \n",
    "\n",
    "[Nick walsh](https://stackoverflow.com/a/56012609) also attests that \"SVD is the more stable solution for preserving data integrity due to rounding inaccuracies as a result of computing the product of your dataset by its tranpose matrix (X*XâŠ¤)\" as well as J.M.'s [LÃ¤uchli matrix example](https://math.stackexchange.com/a/359428) and Amoeba's [Eigenvalue Decomposition comparison](https://stats.stackexchange.com/a/87536).\n",
    "\n",
    "Bear in mind that SVD runs much SLOWER as it is computationally more INTENSIVE.\n",
    "\n",
    "Also, take note of Vaquar Khan's [mllib vs. ml](https://stackoverflow.com/a/58799652) and Mohamed Ali Jamaoui's [pySpark vs. Scipy](https://stackoverflow.com/a/58799652) comparions.\n",
    "\n",
    "In reference to [Elias Abou Haydar and Sergul Aydore's discussion](https://stackoverflow.com/a/38107324), let's import their SVD method as Java wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.common import callMLlibFunc, JavaModelWrapper\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "\n",
    "class SVD(JavaModelWrapper):\n",
    "    \"\"\"Wrapper around the SVD scala case class\"\"\"\n",
    "    @property\n",
    "    def U(self):\n",
    "        \"\"\" Returns a RowMatrix whose columns are the left singular vectors of the SVD if computeU was set to be True.\"\"\"\n",
    "        u = self.call(\"U\")\n",
    "        if u is not None:\n",
    "            return RowMatrix(u)\n",
    "\n",
    "    @property\n",
    "    def s(self):\n",
    "        \"\"\"Returns a DenseVector with singular values in descending order.\"\"\"\n",
    "        return self.call(\"s\")\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        \"\"\" Returns a DenseMatrix whose columns are the right singular vectors of the SVD.\"\"\"\n",
    "        return self.call(\"V\")\n",
    "    \n",
    "def computeSVD(row_matrix, k, computeU=False, rCond=1e-9):\n",
    "    \"\"\"\n",
    "    Computes the singular value decomposition of the RowMatrix.\n",
    "    The given row matrix A of dimension (m X n) is decomposed into U * s * V'T where\n",
    "    * s: DenseVector consisting of square root of the eigenvalues (singular values) in descending order.\n",
    "    * U: (m X k) (left singular vectors) is a RowMatrix whose columns are the eigenvectors of (A X A')\n",
    "    * v: (n X k) (right singular vectors) is a Matrix whose columns are the eigenvectors of (A' X A)\n",
    "    :param k: number of singular values to keep. We might return less than k if there are numerically zero singular values.\n",
    "    :param computeU: Whether of not to compute U. If set to be True, then U is computed by A * V * sigma^-1\n",
    "    :param rCond: the reciprocal condition number. All singular values smaller than rCond * sigma(0) are treated as zero, where sigma(0) is the largest singular value.\n",
    "    :returns: SVD object\n",
    "    \"\"\"\n",
    "    java_model = row_matrix._java_matrix_wrapper.call(\"computeSVD\", int(k), computeU, float(rCond))\n",
    "    return SVD(java_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a4c189fab5af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#df.values.tolist()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vectors' is not defined"
     ]
    }
   ],
   "source": [
    "data = [(Vectors.dense([0.0, 1.0, 0.0, 7.0, 0.0]),), (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),), (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]\n",
    "data\n",
    "#df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.PCA'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "#from pyspark.mllib.linalg import Vectors\n",
    "#from pyspark.sql import sqlContext\n",
    "#from pyspark.mllib.utils import MLUtils\n",
    "#from pyspark.mllib.util import MLUtils\n",
    "\n",
    "\n",
    "RAW_FEATURES = \"raw_feasures\"\n",
    "PCA_FEATURES = \"pca_features\"\n",
    "PCA_FEATURES_NUM = 10\n",
    "\n",
    "#df_pca = spark.createDataFrame(features_in_densevector,[RAW_FEATURES])\n",
    "#type(df_pca) # <class 'pyspark.sql.dataframe.DataFrame'>\n",
    "#df_pca.printSchema()\n",
    "pca_extracted = PCA(k=PCA_FEATURES_NUM, inputCol=RAW_FEATURES, outputCol=PCA_FEATURES)\n",
    "#type(pca_extracted) pyspark.ml.feature.PCA\n",
    "\n",
    "model = pca_extracted.fit(df_pca)\n",
    "#type(model) pyspark.ml.feature.PCAModel\n",
    "\n",
    "features = model.transform(df_pca) # this create a DataFrame with the regular features and pca_features\n",
    "#type(features) pyspark.sql.dataframe.DataFrame\n",
    "features.collect()[0][PCA_FEATURES]\n",
    "\n",
    "# # We can now extract the pca_features to prepare our RowMatrix.\n",
    "pca_features = features.select(PCA_FEATURES).rdd.map(lambda row : row[0])\n",
    "#type(pca_features) # pyspark.rdd.PipelinedRDD\n",
    "\n",
    "\n",
    "#mat = RowMatrix(pca_features)\n",
    "#type(mat) # pyspark.mllib.linalg.distributed.RowMatrix\n",
    "\n",
    "#svd_model = RowMatrix(pca_features).computeSVD(PCA_FEATURES_NUM, True)\n",
    "#svd_model = mat.computeSVD(PCA_FEATURES_NUM, True)\n",
    "# svd_model.U.rows.collect()\n",
    "# # [DenseVector([-0.7071, 0.7071]), DenseVector([-0.7071, -0.7071])]\n",
    "\n",
    "# svd_model.s\n",
    "# # DenseVector([3.4641, 3.1623])\n",
    "\n",
    "# svd_model.V\n",
    "# # DenseMatrix(3, 2, [-0.4082, -0.8165, -0.4082, 0.8944, -0.4472, 0.0], 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Once the RowMatrix is ready we can compute our Singular Value Decomposition\n",
    "\n",
    "\n",
    "# TypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\n",
    "\n",
    "\n",
    "\n",
    "# svd.s\n",
    "# # DenseVector([9.491, 4.6253])\n",
    "# svd.U.rows.collect()\n",
    "# # [DenseVector([0.1129, -0.909]), DenseVector([0.463, 0.4055]), DenseVector([0.8792, -0.0968])]\n",
    "# svd.V\n",
    "# # DenseMatrix(2, 2, [-0.8025, -0.5967, -0.5967, 0.8025], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = RowMatrix(pca_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|        raw_feasures|        pca_features|\n",
      "+--------------------+--------------------+\n",
      "|[6.0,1169.0,4.0,4...|[-1169.0204471024...|\n",
      "|[48.0,5951.0,2.0,...|[-5951.1095244771...|\n",
      "|[12.0,2096.0,2.0,...|[-2096.0315357355...|\n",
      "|[42.0,7882.0,2.0,...|[-7882.0900453892...|\n",
      "|[24.0,4870.0,3.0,...|[-4870.0537625318...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#features.collect()[0][PCA_FEATURES]\n",
    "features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-e44fabc4ba86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET_COL_NAME\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "df[LABEL_COL_NAME].collect().show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'USING column `pca_features` cannot be resolved on the left side of the join. The left-side columns: [target];'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1676.join.\n: org.apache.spark.sql.AnalysisException: USING column `pca_features` cannot be resolved on the left side of the join. The left-side columns: [target];\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$99$$anonfun$apply$71.apply(Analyzer.scala:2347)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$99$$anonfun$apply$71.apply(Analyzer.scala:2347)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$99.apply(Analyzer.scala:2346)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$99.apply(Analyzer.scala:2345)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$commonNaturalJoinProcessing(Analyzer.scala:2345)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$34.applyOrElse(Analyzer.scala:2235)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$$anonfun$apply$34.applyOrElse(Analyzer.scala:2232)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2232)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveNaturalAndUsingJoin$.apply(Analyzer.scala:2231)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)\n\tat org.apache.spark.sql.Dataset.join(Dataset.scala:944)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-dc3a7635dbe6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf_PCA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdf_PCA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_PCA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mPCA_FEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be basestring\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'USING column `pca_features` cannot be resolved on the left side of the join. The left-side columns: [target];'"
     ]
    }
   ],
   "source": [
    "#df_pca = features.withColumn(TARGET_COL_NAME, df[TARGET_COL_NAME])\n",
    "#  [raw_feasures#2657, pca_features#2661, target#971 AS target#2689]\n",
    "\n",
    "#features.collect()[971][RAW_FEATURES]\n",
    "#df_final = df_final.withColumn(PCA_FEATURES, features[PCA_FEATURES])\n",
    "\n",
    "\n",
    "df_target = df_final.select(LABEL_COL_NAME)\n",
    "df_target.cache()\n",
    "\n",
    "df_PCA = features.select(PCA_FEATURES)\n",
    "df_PCA.cache()\n",
    "\n",
    "df_target.join(df_PCA, [PCA_FEATURES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input dataset must be a DataFrame but got <class 'pyspark.rdd.PipelinedRDD'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-87ed7404813e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# type(df_pca.collect()[0][RAW_FEATURES])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpca_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvertVectorColumnsToML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCA_FEATURES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpca_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPCA_FEATURES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/mllib/util.py\u001b[0m in \u001b[0;36mconvertVectorColumnsToML\u001b[0;34m(dataset, *cols)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \"\"\"\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input dataset must be a DataFrame but got {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"convertVectorColumnsToML\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input dataset must be a DataFrame but got <class 'pyspark.rdd.PipelinedRDD'>."
     ]
    }
   ],
   "source": [
    "# TypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\n",
    "#svd = mat.computeSVD(PCA_FEATURES_NUM,True)\n",
    "\n",
    "# from pyspark.mllib.utils import MLUtils\n",
    "from pyspark.mllib.util import MLUtils\n",
    "# df_pca = MLUtils.convertVectorColumnsFromML(df_pca, RAW_FEATURES)\n",
    "#df_pca = MLUtils.convertVectorColumnsToML(df_pca, RAW_FEATURES)\n",
    "# df_pca.collect()[0][RAW_FEATURES]\n",
    "# type(df_pca.collect()[0][RAW_FEATURES])\n",
    "\n",
    "pca_features = MLUtils.convertVectorColumnsToML(pca_features, PCA_FEATURES)\n",
    "type(pca_features.collect()[0][PCA_FEATURES])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1211.computeSVD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 98.0 failed 1 times, most recent failure: Lost task 0.0 in stage 98.0 (TID 131, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1382)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1422)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:232)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:208)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-89966fecb3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputeSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPCA_FEATURES_NUM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/mllib/linalg/distributed.py\u001b[0m in \u001b[0;36mcomputeSVD\u001b[0;34m(self, k, computeU, rCond)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m    343\u001b[0m         j_model = self._java_matrix_wrapper.call(\n\u001b[0;32m--> 344\u001b[0;31m             \"computeSVD\", int(k), bool(computeU), float(rCond))\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSingularValueDecomposition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, name, *a)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;34m\"\"\"Call method of java_model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1211.computeSVD.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 98.0 failed 1 times, most recent failure: Lost task 0.0 in stage 98.0 (TID 131, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1382)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1423)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1422)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:232)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:208)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/to/miniconda3/envs/arcgis/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 83, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:390)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat scala.collection.AbstractIterator.to(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$31.apply(RDD.scala:1409)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#svd = computeSVD(mat,PCA_FEATURES_NUM,True)\n",
    "svd = mat.computeSVD(PCA_FEATURES_NUM,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_test = df_final.randomSplit(0.80, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0: 0\n",
      "_c1: 0\n",
      "_c2: 0\n",
      "_c3: 0\n",
      "_c4: 0\n",
      "_c5: 0\n",
      "_c6: 0\n",
      "_c7: 0\n",
      "_c8: 0\n",
      "_c9: 0\n",
      "_c10: 0\n",
      "_c11: 0\n",
      "_c12: 0\n",
      "_c13: 0\n",
      "_c14: 0\n",
      "_c15: 0\n",
      "_c16: 0\n",
      "_c17: 0\n",
      "_c18: 0\n",
      "_c19: 0\n",
      "_c20: 0\n"
     ]
    }
   ],
   "source": [
    "#df[\"_c0\"].isNull\n",
    "\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "\n",
    "names = df.schema.names\n",
    "\n",
    "for name in names:\n",
    "    trim(df[name])\n",
    "    \n",
    "    print(name + ': ' + str(df.where(df[name].isNull()).count())) # + ' & ' + str(df.where(df[name] == 'NoneType').count())\n",
    "\n",
    "\n",
    "#duh = df.filter(df[\"_c0\"] == ' ').collect()\n",
    "#duh = df.filter(df[\"_c0\"].isNull()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import column\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "for name in names:\n",
    "    suffix = 'A' + str(int(name.replace('_c', '')) + 1)\n",
    "    \n",
    "#    print(suffix + \": \" + name)\n",
    "    # translate(name, 'A' + str(int(name.replace('_c', '')) + 1), '')\n",
    "\n",
    "    #df.name = df.select(regexp_replace(column(name), '^' + suffix, '')) #.alias(name)\n",
    "    #df.name = df.withColumn(name, regexp_replace(name, '^' + suffix, ''))\n",
    "    \n",
    "    \n",
    "    #targetDf = df.withColumn(name, \\\n",
    "    #          when(df[\"session\"] == 0, 999).otherwise(df[\"timestamp1\"]))\n",
    "    \n",
    "# use VectorAssembler for One hot encoding or indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 0, not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6c3b33d0b65c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(10,10), marker = 'o', hist_kwds = {'bins': 10}, s = 60, alpha = 0.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, alpha=.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pair_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_misc.py\u001b[0m in \u001b[0;36mscatter_matrix\u001b[0;34m(frame, alpha, figsize, ax, grid, diagonal, marker, density_kwds, hist_kwds, range_padding, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mhist_kwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist_kwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mrange_padding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange_padding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_matplotlib/misc.py\u001b[0m in \u001b[0;36mscatter_matrix\u001b[0;34m(frame, alpha, figsize, ax, grid, diagonal, marker, density_kwds, hist_kwds, range_padding, **kwds)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnaxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnaxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# no gaps between subplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py\u001b[0m in \u001b[0;36m_subplots\u001b[0;34m(naxes, sharex, sharey, squeeze, subplot_kw, ax, layout, layout_type, **fig_kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# Create first subplot separately, so we can share it if requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0max0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 59\u001b[0;31m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[1;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[1;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 0, not 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_pair_plot = df.select(\"*\").toPandas()\n",
    "\n",
    "#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(10,10), marker = 'o', hist_kwds = {'bins': 10}, s = 60, alpha = 0.8)\n",
    "#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, alpha=.8)\n",
    "g = pd.plotting.scatter_matrix(df_pair_plot, 0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1| 2053|\n",
      "|      0| 4313|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|rate_marriage|count|\n",
      "+-------------+-----+\n",
      "|            1|   99|\n",
      "|            3|  993|\n",
      "|            5| 2684|\n",
      "|            4| 2242|\n",
      "|            2|  348|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('rate_marriage').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+\n",
      "|rate_marriage|affairs|count|\n",
      "+-------------+-------+-----+\n",
      "|            1|      0|   25|\n",
      "|            1|      1|   74|\n",
      "|            2|      0|  127|\n",
      "|            2|      1|  221|\n",
      "|            3|      0|  446|\n",
      "|            3|      1|  547|\n",
      "|            4|      0| 1518|\n",
      "|            4|      1|  724|\n",
      "|            5|      0| 2197|\n",
      "|            5|      1|  487|\n",
      "+-------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('rate_marriage','affairs').count().orderBy('rate_marriage','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|religious|affairs|count|\n",
      "+---------+-------+-----+\n",
      "|        1|      0|  613|\n",
      "|        1|      1|  408|\n",
      "|        2|      0| 1448|\n",
      "|        2|      1|  819|\n",
      "|        3|      0| 1715|\n",
      "|        3|      1|  707|\n",
      "|        4|      0|  537|\n",
      "|        4|      1|  119|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('religious','affairs').count().orderBy('religious','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "|children|affairs|count|\n",
      "+--------+-------+-----+\n",
      "|     0.0|      0| 1912|\n",
      "|     0.0|      1|  502|\n",
      "|     1.0|      0|  747|\n",
      "|     1.0|      1|  412|\n",
      "|     2.0|      0|  873|\n",
      "|     2.0|      1|  608|\n",
      "|     3.0|      0|  460|\n",
      "|     3.0|      1|  321|\n",
      "|     4.0|      0|  197|\n",
      "|     4.0|      1|  131|\n",
      "|     5.5|      0|  124|\n",
      "|     5.5|      1|   79|\n",
      "+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('children','affairs').count().orderBy('children','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "|affairs|avg(rate_marriage)|          avg(age)|  avg(yrs_married)|     avg(children)|    avg(religious)|avg(affairs)|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "|      1|3.6473453482708234|30.537018996590355|11.152459814905017|1.7289332683877252| 2.261568436434486|         1.0|\n",
      "|      0| 4.329700904242986| 28.39067934152562| 7.989334569904939|1.2388128912589844|2.5045212149316023|         0.0|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('affairs').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['rate_marriage', 'age', 'yrs_married', 'children', 'religious'], outputCol=\"features\")\n",
    "df = df_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rate_marriage: integer (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- yrs_married: double (nullable = true)\n",
      " |-- children: double (nullable = true)\n",
      " |-- religious: integer (nullable = true)\n",
      " |-- affairs: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------+\n",
      "|features               |affairs|\n",
      "+-----------------------+-------+\n",
      "|[5.0,32.0,6.0,1.0,3.0] |0      |\n",
      "|[4.0,22.0,2.5,0.0,2.0] |0      |\n",
      "|[3.0,32.0,9.0,3.0,3.0] |1      |\n",
      "|[3.0,27.0,13.0,3.0,1.0]|1      |\n",
      "|[4.0,22.0,2.5,0.0,1.0] |1      |\n",
      "|[4.0,37.0,16.5,4.0,3.0]|1      |\n",
      "|[5.0,27.0,9.0,1.0,1.0] |1      |\n",
      "|[4.0,27.0,9.0,0.0,2.0] |1      |\n",
      "|[5.0,37.0,23.0,5.5,2.0]|1      |\n",
      "|[5.0,37.0,23.0,5.5,2.0]|1      |\n",
      "+-----------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['features','affairs']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data for building model\n",
    "model_df=df.select(['features','affairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=model_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1| 1574|\n",
      "|      0| 3226|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1|  479|\n",
      "|      0| 1087|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier=RandomForestClassifier(labelCol='affairs',numTrees=50).fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions=rf_classifier.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|affairs|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.0,22.0,2.5,1.0...|      1|[18.7524598757082...|[0.37504919751416...|       1.0|\n",
      "|[1.0,27.0,2.5,0.0...|      1|[21.0021478623554...|[0.42004295724710...|       1.0|\n",
      "|[1.0,27.0,6.0,0.0...|      0|[20.1166611727778...|[0.40233322345555...|       1.0|\n",
      "|[1.0,27.0,6.0,2.0...|      1|[19.0905206218080...|[0.38181041243616...|       1.0|\n",
      "|[1.0,27.0,6.0,3.0...|      0|[16.3348579592130...|[0.32669715918426...|       1.0|\n",
      "|[1.0,27.0,9.0,4.0...|      0|[13.3128973003485...|[0.26625794600697...|       1.0|\n",
      "|[1.0,32.0,13.0,0....|      1|[16.7812008990910...|[0.33562401798182...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[12.6966189294366...|[0.25393237858873...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[12.6766379097881...|[0.25353275819576...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[16.5966383870776...|[0.33193276774155...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[16.5966383870776...|[0.33193276774155...|       1.0|\n",
      "|[1.0,32.0,16.5,3....|      1|[18.0962335004510...|[0.36192467000902...|       1.0|\n",
      "|[1.0,37.0,16.5,1....|      1|[17.1671412215484...|[0.34334282443096...|       1.0|\n",
      "|[1.0,37.0,16.5,2....|      1|[16.9033235593205...|[0.33806647118641...|       1.0|\n",
      "|[1.0,37.0,23.0,4....|      1|[12.9328949963349...|[0.25865789992669...|       1.0|\n",
      "|[1.0,37.0,23.0,5....|      1|[19.2882486885513...|[0.38576497377102...|       1.0|\n",
      "|[1.0,42.0,16.5,5....|      1|[14.7895576971207...|[0.29579115394241...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[13.1159769754090...|[0.26231953950818...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[16.8901830088537...|[0.33780366017707...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[16.8901830088537...|[0.33780366017707...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 1261|\n",
      "|       1.0|  305|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-------+----------+\n",
      "|probability                             |affairs|prediction|\n",
      "+----------------------------------------+-------+----------+\n",
      "|[0.37504919751416455,0.6249508024858356]|1      |1.0       |\n",
      "|[0.42004295724710805,0.579957042752892] |1      |1.0       |\n",
      "|[0.40233322345555694,0.597666776544443] |0      |1.0       |\n",
      "|[0.3818104124361619,0.6181895875638381] |1      |1.0       |\n",
      "|[0.32669715918426007,0.6733028408157399]|0      |1.0       |\n",
      "|[0.26625794600697006,0.7337420539930299]|0      |1.0       |\n",
      "|[0.3356240179818214,0.6643759820181787] |1      |1.0       |\n",
      "|[0.25393237858873335,0.7460676214112667]|1      |1.0       |\n",
      "|[0.2535327581957624,0.7464672418042376] |1      |1.0       |\n",
      "|[0.3319327677415531,0.6680672322584469] |1      |1.0       |\n",
      "+----------------------------------------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.select(['probability','affairs','prediction']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_accuracy=MulticlassClassificationEvaluator(labelCol='affairs',metricName='accuracy').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of RF on test data is 73%\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of RF on test data is {0:.0%}'.format(rf_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7279693486590039\n"
     ]
    }
   ],
   "source": [
    "print(rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_precision=MulticlassClassificationEvaluator(labelCol='affairs',metricName='weightedPrecision').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision rate on test data is 71%\n"
     ]
    }
   ],
   "source": [
    "print('The precision rate on test data is {0:.0%}'.format(rf_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7085017563673452"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_auc=BinaryClassificationEvaluator(labelCol='affairs').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421702296835062\n"
     ]
    }
   ],
   "source": [
    "print(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.5536, 1: 0.0364, 2: 0.2358, 3: 0.0803, 4: 0.0939})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': [{'idx': 0, 'name': 'rate_marriage'},\n",
       "  {'idx': 1, 'name': 'age'},\n",
       "  {'idx': 2, 'name': 'yrs_married'},\n",
       "  {'idx': 3, 'name': 'children'},\n",
       "  {'idx': 4, 'name': 'religious'}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.save(\"/home/jovyan/work/RF_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassificationModel.load(\"/home/jovyan/work/RF_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preditions=rf.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|affairs|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.0,22.0,2.5,1.0...|      1|[188.676639360932...|[0.37735327872186...|       1.0|\n",
      "|[1.0,27.0,2.5,0.0...|      1|[195.425833792250...|[0.39085166758450...|       1.0|\n",
      "|[1.0,27.0,6.0,0.0...|      0|[193.138478579040...|[0.38627695715808...|       1.0|\n",
      "|[1.0,27.0,6.0,2.0...|      1|[185.424877645536...|[0.37084975529107...|       1.0|\n",
      "|[1.0,27.0,6.0,3.0...|      0|[164.685852316351...|[0.32937170463270...|       1.0|\n",
      "|[1.0,27.0,9.0,4.0...|      0|[142.006095001922...|[0.28401219000384...|       1.0|\n",
      "|[1.0,32.0,13.0,0....|      1|[176.885399312490...|[0.35377079862498...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[128.585405941664...|[0.25717081188332...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[126.963464206019...|[0.25392692841203...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[153.429839787005...|[0.30685967957401...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[153.429839787005...|[0.30685967957401...|       1.0|\n",
      "|[1.0,32.0,16.5,3....|      1|[167.493344562280...|[0.33498668912456...|       1.0|\n",
      "|[1.0,37.0,16.5,1....|      1|[150.090425556233...|[0.30018085111246...|       1.0|\n",
      "|[1.0,37.0,16.5,2....|      1|[154.326506925977...|[0.30865301385195...|       1.0|\n",
      "|[1.0,37.0,23.0,4....|      1|[123.559481897677...|[0.24711896379535...|       1.0|\n",
      "|[1.0,37.0,23.0,5....|      1|[192.317134975430...|[0.38463426995086...|       1.0|\n",
      "|[1.0,42.0,16.5,5....|      1|[151.350101320899...|[0.30270020264179...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[126.937590310819...|[0.25387518062163...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[156.647804216314...|[0.31329560843262...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[156.647804216314...|[0.31329560843262...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_preditions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
