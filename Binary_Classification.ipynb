{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'local[*]' - instructing to use all the cores (*) on a local machine\n",
    "# 'appName' - will be given a random name unless specified, like Docker container image name\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder. \\\n",
    "        master('local[*]').\\\n",
    "        appName('credit risk').\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source CSV file must reside in the same folder\n",
    "# The 'german.data-numeric' file delimits columns with 2 spaces ('  ')\n",
    "#   Without data scrubbing, \"IllegalArgumentException: 'Delimiter cannot be more than one character:   '\"\n",
    "df = spark.read.csv('german.data',sep=' ',inferSchema=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 21)\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      "\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|_c0|_c1|_c2|_c3| _c4|_c5|_c6|_c7|_c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|   4|A121|  67|A143|A152|   2|A173|   1|A192|A201|   1|\n",
      "|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|   2|A121|  22|A143|A152|   1|A173|   1|A191|A201|   2|\n",
      "|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|   3|A121|  49|A143|A152|   1|A172|   2|A191|A201|   1|\n",
      "|A11| 42|A32|A42|7882|A61|A74|  2|A93|A103|   4|A122|  45|A143|A153|   1|A173|   2|A191|A201|   1|\n",
      "|A11| 24|A33|A40|4870|A61|A73|  3|A93|A101|   4|A124|  53|A143|A153|   2|A173|   2|A191|A201|   2|\n",
      "+---+---+---+---+----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((df.count(),len(df.columns)))\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the columns with numbers are \"**ordinal**\" i.e., the magnitude in each column/feature means \"something\" (e.g., age, duration, credit amount and installment rate). Thus, transformation and encoding for them is not needed and they will be converted to non-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.withColumnRenamed('1', '_c0')\n",
    "df= df.withColumnRenamed('2', '_c1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ShortType # a signed 16-bit integer vs. IntegerType (a signed 32-bit integer)\n",
    "\n",
    "col_indices_of_ordinal_numbers = [2, 5, 8, 11, 13, 16, 18, 21]\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # change the name of each column e.g., \"_c0\" -> \"1\"\n",
    "    new_col_num = int(col_name.replace('_c', '')) + 1\n",
    "    df = df.withColumnRenamed(col_name, str(new_col_num))\n",
    "    \n",
    "    if new_col_num in col_indices_of_ordinal_numbers:  \n",
    "        df = df.withColumn(str(new_col_num), df[str(new_col_num)].cast(ShortType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "|  1|  2|  3|  4|   5|  6|  7|  8|  9|  10| 11|  12| 13|  14|  15| 16|  17| 18|  19|  20| 21|\n",
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "|A11|  6|A34|A43|1169|A65|A75|  4|A93|A101|  4|A121| 67|A143|A152|  2|A173|  1|A192|A201|  1|\n",
      "|A12| 48|A32|A43|5951|A61|A73|  2|A92|A101|  2|A121| 22|A143|A152|  1|A173|  1|A191|A201|  2|\n",
      "|A14| 12|A34|A46|2096|A61|A74|  2|A93|A101|  3|A121| 49|A143|A152|  1|A172|  2|A191|A201|  1|\n",
      "|A11| 42|A32|A42|7882|A61|A74|  2|A93|A103|  4|A122| 45|A143|A153|  1|A173|  2|A191|A201|  1|\n",
      "|A11| 24|A33|A40|4870|A61|A73|  3|A93|A101|  4|A124| 53|A143|A153|  2|A173|  2|A191|A201|  2|\n",
      "+---+---+---+---+----+---+---+---+---+----+---+----+---+----+----+---+----+---+----+----+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 'string'),\n",
       " ('2', 'smallint'),\n",
       " ('3', 'string'),\n",
       " ('4', 'string'),\n",
       " ('5', 'smallint'),\n",
       " ('6', 'string'),\n",
       " ('7', 'string'),\n",
       " ('8', 'smallint'),\n",
       " ('9', 'string'),\n",
       " ('10', 'string'),\n",
       " ('11', 'smallint'),\n",
       " ('12', 'string'),\n",
       " ('13', 'smallint'),\n",
       " ('14', 'string'),\n",
       " ('15', 'string'),\n",
       " ('16', 'smallint'),\n",
       " ('17', 'string'),\n",
       " ('18', 'smallint'),\n",
       " ('19', 'string'),\n",
       " ('20', 'string'),\n",
       " ('21', 'smallint')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show(5)\n",
    "df.dtypes\n",
    "#df = df.withColumnRenamed('21', 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "\n",
    "COL_SUFFIX_IDX = \"_INDEX\"\n",
    "COL_SUFFIX_VEC = \"_VEC\"\n",
    "TARGET_COL_NAME = \"target\"\n",
    "\n",
    "df = df.withColumnRenamed(str(len(df.columns)), TARGET_COL_NAME)\n",
    "\n",
    "# Unforutnately need to loop through all the columns, as there is no 'dtype' attribute for column\n",
    "def get_dtype(df, col_name):\n",
    "    return [dtype for name, dtype in df.dtypes if name == col_name][0]\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # Transform and encode all the non-string columns EXCEPT the last column of target/label \n",
    "    if (get_dtype(df, col_name) == 'string') and (col_name != TARGET_COL_NAME):\n",
    "        # STAGE 1 of 2: TRANSFORMING\n",
    "\n",
    "        # Class StringIndexer (an abstract of Estimator)\n",
    "        stringIndexer = StringIndexer(inputCol=col_name, outputCol=col_name + COL_SUFFIX_IDX)\n",
    "\n",
    "        # Fit a model to the input dataset, i.e., mapping categorical values to indices\n",
    "        model = stringIndexer.fit(df)\n",
    "\n",
    "        # transform converts the words column into feature vectors, adding a new column with those vectors to the DataFrame. \n",
    "        col_indexed_df = model.transform(df)\n",
    "\n",
    "        # STAGE 2 of 2: ENCODING\n",
    "        # Class OneHotEncoder\n",
    "        oneHotEncoder = OneHotEncoder(inputCol=col_name + COL_SUFFIX_IDX, outputCol=col_name + COL_SUFFIX_VEC, dropLast=False)\n",
    " \n",
    "        # NO need to fit()\n",
    "        df = oneHotEncoder.transform(col_indexed_df) # Assgin back to 'df', using the col_indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[1: string, 2: smallint, 3: string, 4: string, 5: smallint, 6: string, 7: string, 8: smallint, 9: string, 10: string, 11: smallint, 12: string, 13: smallint, 14: string, 15: string, 16: smallint, 17: string, 18: smallint, 19: string, 20: string, target: smallint, 1_INDEX: double, 1_VEC: vector, 3_INDEX: double, 3_VEC: vector, 4_INDEX: double, 4_VEC: vector, 6_INDEX: double, 6_VEC: vector, 7_INDEX: double, 7_VEC: vector, 9_INDEX: double, 9_VEC: vector, 10_INDEX: double, 10_VEC: vector, 12_INDEX: double, 12_VEC: vector, 14_INDEX: double, 14_VEC: vector, 15_INDEX: double, 15_VEC: vector, 17_INDEX: double, 17_VEC: vector, 19_INDEX: double, 19_VEC: vector, 20_INDEX: double, 20_VEC: vector]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema # terser output from that of `df.printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "FEATURE_VECTOR_COLUMN_NAME = \"features\"\n",
    "feature_columns = []\n",
    "\n",
    "for col_name in df.schema.names:\n",
    "    # Transform and encod all the non-string columns EXCEPT the last column of target/label\n",
    "    if ((get_dtype(df, col_name) == 'smallint') or (get_dtype(df, col_name) == 'vector')) \\\n",
    "        and (col_name != TARGET_COL_NAME):\n",
    "\n",
    "        feature_columns.append(col_name)\n",
    "\n",
    "#feature_columns\n",
    "vectorAssembler = VectorAssembler(inputCols=feature_columns, outputCol=FEATURE_VECTOR_COLUMN_NAME) # inputCols, NOT inputCol\n",
    "df = vectorAssembler.transform(df)\n",
    "\n",
    "# Spark mandates all the features are bunched up into a single vector for each row, in a single column plus target column\n",
    "df_final = df.select([FEATURE_VECTOR_COLUMN_NAME, TARGET_COL_NAME])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features|target|\n",
      "+--------------------+------+\n",
      "|(61,[0,1,2,3,4,5,...|     1|\n",
      "|(61,[0,1,2,3,4,5,...|     2|\n",
      "|(61,[0,1,2,3,4,5,...|     1|\n",
      "|(61,[0,1,2,3,4,5,...|     1|\n",
      "|(61,[0,1,2,3,4,5,...|     2|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(61, {0: 6.0, 1: 1169.0, 2: 4.0, 3: 4.0, 4: 67.0, 5: 2.0, 6: 1.0, 8: 1.0, 12: 1.0, 16: 1.0, 27: 1.0, 32: 1.0, 36: 1.0, 40: 1.0, 44: 1.0, 47: 1.0, 50: 1.0, 53: 1.0, 58: 1.0, 59: 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.collect()[0]['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 [ 0  1  2  3  4  5  6  8 12 16 27 32 36 40 44 47 50 53 58 59] 20\n"
     ]
    }
   ],
   "source": [
    "t = df_final.collect()[0]['features']\n",
    "print(t[7], t.indices, t.indices.shape[0])\n",
    "\n",
    "#19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.indices[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[int(t.indices[18])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5\n",
      "8\n",
      "11\n",
      "13\n",
      "16\n",
      "18\n",
      "1_VEC\n",
      "3_VEC\n",
      "4_VEC\n",
      "6_VEC\n",
      "7_VEC\n",
      "9_VEC\n",
      "10_VEC\n",
      "12_VEC\n",
      "14_VEC\n",
      "15_VEC\n",
      "17_VEC\n",
      "19_VEC\n",
      "20_VEC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function list.count(value, /)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col_name in feature_columns:\n",
    "    print(col_name)\n",
    "feature_columns.count\n",
    "#20 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1='A11', 2=6, 3='A34', 4='A43', 5=1169, 6='A65', 7='A75', 8=4, 9='A93', 10='A101', 11=4, 12='A121', 13=67, 14='A143', 15='A152', 16=2, 17='A173', 18=1, 19='A192', 20='A201', target=1, 1_INDEX=1.0, 1_VEC=SparseVector(3, {1: 1.0}), 3_INDEX=1.0, 3_VEC=SparseVector(4, {1: 1.0}), 4_INDEX=0.0, 4_VEC=SparseVector(9, {0: 1.0}), 6_INDEX=1.0, 6_VEC=SparseVector(4, {1: 1.0}), 7_INDEX=1.0, 7_VEC=SparseVector(4, {1: 1.0}), 9_INDEX=0.0, 9_VEC=SparseVector(3, {0: 1.0}), 10_INDEX=0.0, 10_VEC=SparseVector(2, {0: 1.0}), 12_INDEX=1.0, 12_VEC=SparseVector(3, {1: 1.0}), 14_INDEX=0.0, 14_VEC=SparseVector(2, {0: 1.0}), 15_INDEX=0.0, 15_VEC=SparseVector(2, {0: 1.0}), 17_INDEX=0.0, 17_VEC=SparseVector(3, {0: 1.0}), 19_INDEX=1.0, 19_VEC=SparseVector(1, {}), 20_INDEX=0.0, 20_VEC=SparseVector(1, {0: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1169\n",
      "4\n",
      "4\n",
      "67\n",
      "2\n",
      "1\n",
      "(4,[1],[1.0])\n",
      "(5,[1],[1.0])\n",
      "(10,[0],[1.0])\n",
      "(5,[1],[1.0])\n",
      "(5,[1],[1.0])\n",
      "(4,[0],[1.0])\n",
      "(3,[0],[1.0])\n",
      "(4,[1],[1.0])\n",
      "(3,[0],[1.0])\n",
      "(3,[0],[1.0])\n",
      "(4,[0],[1.0])\n",
      "(2,[1],[1.0])\n",
      "(2,[0],[1.0])\n"
     ]
    }
   ],
   "source": [
    "for col_name in feature_columns:\n",
    "    print(df.collect()[0][col_name])\n",
    "#20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1169, 4, 4, 67, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "\n",
    "for idx in range(t.indices.shape[0]):\n",
    "    # without int(), TypeError: Indices must be of type integer, got type <class 'numpy.int32'>\n",
    "    key = int(t.indices[idx])\n",
    "    \n",
    "    features_list.append(int(t[key]))\n",
    "\n",
    "\n",
    "    \n",
    "# features_values = tuple(features_list)\n",
    "print(tuple(features_list))\n",
    "\n",
    "\n",
    "# WHy 19????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6, 1169, 4, 4, 67, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = []\n",
    "t2.append(tuple(features_list))\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    ">>> l = [('Alice', 1)]\n",
    ">>> spark.createDataFrame(l).collect()\n",
    "[Row(_1=u'Alice', _2=1)]\n",
    ">>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
    "[Row(name=u'Alice', age=1)]\n",
    "\n",
    ">>> d = [{'name': 'Alice', 'age': 1}]\n",
    ">>> spark.createDataFrame(d).collect()\n",
    "[Row(age=1, name=u'Alice')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(1='A11', 2=6, 3='A34', 4='A43', 5=1169, 6='A65', 7='A75', 8=4, 9='A93', 10='A101', 11=4, 12='A121', 13=67, 14='A143', 15='A152', 16=2, 17='A173', 18=1, 19='A192', 20='A201', target=1, 1_INDEX=1.0, 1_VEC=SparseVector(3, {1: 1.0}), 3_INDEX=1.0, 3_VEC=SparseVector(4, {1: 1.0}), 4_INDEX=0.0, 4_VEC=SparseVector(9, {0: 1.0}), 6_INDEX=1.0, 6_VEC=SparseVector(4, {1: 1.0}), 7_INDEX=1.0, 7_VEC=SparseVector(4, {1: 1.0}), 9_INDEX=0.0, 9_VEC=SparseVector(3, {0: 1.0}), 10_INDEX=0.0, 10_VEC=SparseVector(2, {0: 1.0}), 12_INDEX=1.0, 12_VEC=SparseVector(3, {1: 1.0}), 14_INDEX=0.0, 14_VEC=SparseVector(2, {0: 1.0}), 15_INDEX=0.0, 15_VEC=SparseVector(2, {0: 1.0}), 17_INDEX=0.0, 17_VEC=SparseVector(3, {0: 1.0}), 19_INDEX=1.0, 19_VEC=SparseVector(1, {}), 20_INDEX=0.0, 20_VEC=SparseVector(1, {0: 1.0}), features=SparseVector(48, {0: 6.0, 1: 1169.0, 2: 4.0, 3: 4.0, 4: 67.0, 5: 2.0, 6: 1.0, 8: 1.0, 11: 1.0, 14: 1.0, 24: 1.0, 28: 1.0, 31: 1.0, 34: 1.0, 37: 1.0, 39: 1.0, 41: 1.0, 43: 1.0, 47: 1.0}))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparseVector**, that is! \n",
    "\n",
    "The preivous `for` loop iterates all the columns and puts column data only in 'smallint' and 'vector' datatypes into the 'feature_columns' list. If you closely look at the first row [0] from left to right above, the code picks up 'smallint' first i.e., the column numbers in the col_indices_of_ordinal_numbers list, then the added vector-type columns with suffix of \"_VEC\".\n",
    "\n",
    "2=6, 5=1169, 8=4, 11=4, 13=67, 16=2, 18=1, target=1,\n",
    "{0: 6.0, 1: 1169.0, 2: 4.0, 3: 4.0, 4: 67.0, 5: 2.0, 6: 1.0, 8: 1.0,                       13: 1.0, \n",
    "\n",
    "GET RID OF TARGET COLUMN target = 1\n",
    "\n",
    "\n",
    "\n",
    "This means there are 968 (967 + 1) unique values all across the features, of which this particular sample, the first row [0], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_test = df_final.randomSplit(0.80, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_c0: 0\n",
      "_c1: 0\n",
      "_c2: 0\n",
      "_c3: 0\n",
      "_c4: 0\n",
      "_c5: 0\n",
      "_c6: 0\n",
      "_c7: 0\n",
      "_c8: 0\n",
      "_c9: 0\n",
      "_c10: 0\n",
      "_c11: 0\n",
      "_c12: 0\n",
      "_c13: 0\n",
      "_c14: 0\n",
      "_c15: 0\n",
      "_c16: 0\n",
      "_c17: 0\n",
      "_c18: 0\n",
      "_c19: 0\n",
      "_c20: 0\n"
     ]
    }
   ],
   "source": [
    "#df[\"_c0\"].isNull\n",
    "\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "\n",
    "names = df.schema.names\n",
    "\n",
    "for name in names:\n",
    "    trim(df[name])\n",
    "    \n",
    "    print(name + ': ' + str(df.where(df[name].isNull()).count())) # + ' & ' + str(df.where(df[name] == 'NoneType').count())\n",
    "\n",
    "\n",
    "#duh = df.filter(df[\"_c0\"] == ' ').collect()\n",
    "#duh = df.filter(df[\"_c0\"].isNull()).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string, _c12: string, _c13: string, _c14: string, _c15: string, _c16: string, _c17: string, _c18: string, _c19: string, _c20: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import column\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "for name in names:\n",
    "    suffix = 'A' + str(int(name.replace('_c', '')) + 1)\n",
    "    \n",
    "#    print(suffix + \": \" + name)\n",
    "    # translate(name, 'A' + str(int(name.replace('_c', '')) + 1), '')\n",
    "\n",
    "    #df.name = df.select(regexp_replace(column(name), '^' + suffix, '')) #.alias(name)\n",
    "    #df.name = df.withColumn(name, regexp_replace(name, '^' + suffix, ''))\n",
    "    \n",
    "    \n",
    "    #targetDf = df.withColumn(name, \\\n",
    "    #          when(df[\"session\"] == 0, 999).otherwise(df[\"timestamp1\"]))\n",
    "    \n",
    "# use VectorAssembler for One hot encoding or indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num must be 1 <= num <= 0, not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6c3b33d0b65c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(10,10), marker = 'o', hist_kwds = {'bins': 10}, s = 60, alpha = 0.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, alpha=.8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_pair_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_misc.py\u001b[0m in \u001b[0;36mscatter_matrix\u001b[0;34m(frame, alpha, figsize, ax, grid, diagonal, marker, density_kwds, hist_kwds, range_padding, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mhist_kwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist_kwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mrange_padding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange_padding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     )\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_matplotlib/misc.py\u001b[0m in \u001b[0;36mscatter_matrix\u001b[0;34m(frame, alpha, figsize, ax, grid, diagonal, marker, density_kwds, hist_kwds, range_padding, **kwds)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mnaxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnaxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# no gaps between subplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/pandas/plotting/_matplotlib/tools.py\u001b[0m in \u001b[0;36m_subplots\u001b[0;34m(naxes, sharex, sharey, squeeze, subplot_kw, ax, layout, layout_type, **fig_kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;31m# Create first subplot separately, so we can share it if requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m     \u001b[0max0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msubplot_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1414\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/arcgis/lib/python3.7/site-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 59\u001b[0;31m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[1;32m     60\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[1;32m     61\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
      "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 0, not 1"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_pair_plot = df.select(\"*\").toPandas()\n",
    "\n",
    "#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(10,10), marker = 'o', hist_kwds = {'bins': 10}, s = 60, alpha = 0.8)\n",
    "#g = pd.plotting.scatter_matrix(df_pair_plot, figsize=(15, 15), marker='o', hist_kwds={'bins': 20}, alpha=.8)\n",
    "g = pd.plotting.scatter_matrix(df_pair_plot, 0.8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1| 2053|\n",
      "|      0| 4313|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|rate_marriage|count|\n",
      "+-------------+-----+\n",
      "|            1|   99|\n",
      "|            3|  993|\n",
      "|            5| 2684|\n",
      "|            4| 2242|\n",
      "|            2|  348|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('rate_marriage').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+\n",
      "|rate_marriage|affairs|count|\n",
      "+-------------+-------+-----+\n",
      "|            1|      0|   25|\n",
      "|            1|      1|   74|\n",
      "|            2|      0|  127|\n",
      "|            2|      1|  221|\n",
      "|            3|      0|  446|\n",
      "|            3|      1|  547|\n",
      "|            4|      0| 1518|\n",
      "|            4|      1|  724|\n",
      "|            5|      0| 2197|\n",
      "|            5|      1|  487|\n",
      "+-------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('rate_marriage','affairs').count().orderBy('rate_marriage','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+\n",
      "|religious|affairs|count|\n",
      "+---------+-------+-----+\n",
      "|        1|      0|  613|\n",
      "|        1|      1|  408|\n",
      "|        2|      0| 1448|\n",
      "|        2|      1|  819|\n",
      "|        3|      0| 1715|\n",
      "|        3|      1|  707|\n",
      "|        4|      0|  537|\n",
      "|        4|      1|  119|\n",
      "+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('religious','affairs').count().orderBy('religious','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+\n",
      "|children|affairs|count|\n",
      "+--------+-------+-----+\n",
      "|     0.0|      0| 1912|\n",
      "|     0.0|      1|  502|\n",
      "|     1.0|      0|  747|\n",
      "|     1.0|      1|  412|\n",
      "|     2.0|      0|  873|\n",
      "|     2.0|      1|  608|\n",
      "|     3.0|      0|  460|\n",
      "|     3.0|      1|  321|\n",
      "|     4.0|      0|  197|\n",
      "|     4.0|      1|  131|\n",
      "|     5.5|      0|  124|\n",
      "|     5.5|      1|   79|\n",
      "+--------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('children','affairs').count().orderBy('children','affairs','count',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "|affairs|avg(rate_marriage)|          avg(age)|  avg(yrs_married)|     avg(children)|    avg(religious)|avg(affairs)|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "|      1|3.6473453482708234|30.537018996590355|11.152459814905017|1.7289332683877252| 2.261568436434486|         1.0|\n",
      "|      0| 4.329700904242986| 28.39067934152562| 7.989334569904939|1.2388128912589844|2.5045212149316023|         0.0|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('affairs').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['rate_marriage', 'age', 'yrs_married', 'children', 'religious'], outputCol=\"features\")\n",
    "df = df_assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rate_marriage: integer (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- yrs_married: double (nullable = true)\n",
      " |-- children: double (nullable = true)\n",
      " |-- religious: integer (nullable = true)\n",
      " |-- affairs: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------+\n",
      "|features               |affairs|\n",
      "+-----------------------+-------+\n",
      "|[5.0,32.0,6.0,1.0,3.0] |0      |\n",
      "|[4.0,22.0,2.5,0.0,2.0] |0      |\n",
      "|[3.0,32.0,9.0,3.0,3.0] |1      |\n",
      "|[3.0,27.0,13.0,3.0,1.0]|1      |\n",
      "|[4.0,22.0,2.5,0.0,1.0] |1      |\n",
      "|[4.0,37.0,16.5,4.0,3.0]|1      |\n",
      "|[5.0,27.0,9.0,1.0,1.0] |1      |\n",
      "|[4.0,27.0,9.0,0.0,2.0] |1      |\n",
      "|[5.0,37.0,23.0,5.5,2.0]|1      |\n",
      "|[5.0,37.0,23.0,5.5,2.0]|1      |\n",
      "+-----------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['features','affairs']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select data for building model\n",
    "model_df=df.select(['features','affairs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df,test_df=model_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1| 1574|\n",
      "|      0| 3226|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|affairs|count|\n",
      "+-------+-----+\n",
      "|      1|  479|\n",
      "|      0| 1087|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('affairs').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier=RandomForestClassifier(labelCol='affairs',numTrees=50).fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions=rf_classifier.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|affairs|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.0,22.0,2.5,1.0...|      1|[18.7524598757082...|[0.37504919751416...|       1.0|\n",
      "|[1.0,27.0,2.5,0.0...|      1|[21.0021478623554...|[0.42004295724710...|       1.0|\n",
      "|[1.0,27.0,6.0,0.0...|      0|[20.1166611727778...|[0.40233322345555...|       1.0|\n",
      "|[1.0,27.0,6.0,2.0...|      1|[19.0905206218080...|[0.38181041243616...|       1.0|\n",
      "|[1.0,27.0,6.0,3.0...|      0|[16.3348579592130...|[0.32669715918426...|       1.0|\n",
      "|[1.0,27.0,9.0,4.0...|      0|[13.3128973003485...|[0.26625794600697...|       1.0|\n",
      "|[1.0,32.0,13.0,0....|      1|[16.7812008990910...|[0.33562401798182...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[12.6966189294366...|[0.25393237858873...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[12.6766379097881...|[0.25353275819576...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[16.5966383870776...|[0.33193276774155...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[16.5966383870776...|[0.33193276774155...|       1.0|\n",
      "|[1.0,32.0,16.5,3....|      1|[18.0962335004510...|[0.36192467000902...|       1.0|\n",
      "|[1.0,37.0,16.5,1....|      1|[17.1671412215484...|[0.34334282443096...|       1.0|\n",
      "|[1.0,37.0,16.5,2....|      1|[16.9033235593205...|[0.33806647118641...|       1.0|\n",
      "|[1.0,37.0,23.0,4....|      1|[12.9328949963349...|[0.25865789992669...|       1.0|\n",
      "|[1.0,37.0,23.0,5....|      1|[19.2882486885513...|[0.38576497377102...|       1.0|\n",
      "|[1.0,42.0,16.5,5....|      1|[14.7895576971207...|[0.29579115394241...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[13.1159769754090...|[0.26231953950818...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[16.8901830088537...|[0.33780366017707...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[16.8901830088537...|[0.33780366017707...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       0.0| 1261|\n",
      "|       1.0|  305|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-------+----------+\n",
      "|probability                             |affairs|prediction|\n",
      "+----------------------------------------+-------+----------+\n",
      "|[0.37504919751416455,0.6249508024858356]|1      |1.0       |\n",
      "|[0.42004295724710805,0.579957042752892] |1      |1.0       |\n",
      "|[0.40233322345555694,0.597666776544443] |0      |1.0       |\n",
      "|[0.3818104124361619,0.6181895875638381] |1      |1.0       |\n",
      "|[0.32669715918426007,0.6733028408157399]|0      |1.0       |\n",
      "|[0.26625794600697006,0.7337420539930299]|0      |1.0       |\n",
      "|[0.3356240179818214,0.6643759820181787] |1      |1.0       |\n",
      "|[0.25393237858873335,0.7460676214112667]|1      |1.0       |\n",
      "|[0.2535327581957624,0.7464672418042376] |1      |1.0       |\n",
      "|[0.3319327677415531,0.6680672322584469] |1      |1.0       |\n",
      "+----------------------------------------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions.select(['probability','affairs','prediction']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_accuracy=MulticlassClassificationEvaluator(labelCol='affairs',metricName='accuracy').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of RF on test data is 73%\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy of RF on test data is {0:.0%}'.format(rf_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7279693486590039\n"
     ]
    }
   ],
   "source": [
    "print(rf_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_precision=MulticlassClassificationEvaluator(labelCol='affairs',metricName='weightedPrecision').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision rate on test data is 71%\n"
     ]
    }
   ],
   "source": [
    "print('The precision rate on test data is {0:.0%}'.format(rf_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7085017563673452"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_auc=BinaryClassificationEvaluator(labelCol='affairs').evaluate(rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7421702296835062\n"
     ]
    }
   ],
   "source": [
    "print(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(5, {0: 0.5536, 1: 0.0364, 2: 0.2358, 3: 0.0803, 4: 0.0939})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numeric': [{'idx': 0, 'name': 'rate_marriage'},\n",
       "  {'idx': 1, 'name': 'age'},\n",
       "  {'idx': 2, 'name': 'yrs_married'},\n",
       "  {'idx': 3, 'name': 'children'},\n",
       "  {'idx': 4, 'name': 'religious'}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema[\"features\"].metadata[\"ml_attr\"][\"attrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.save(\"/home/jovyan/work/RF_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassificationModel.load(\"/home/jovyan/work/RF_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preditions=rf.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|            features|affairs|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "|[1.0,22.0,2.5,1.0...|      1|[188.676639360932...|[0.37735327872186...|       1.0|\n",
      "|[1.0,27.0,2.5,0.0...|      1|[195.425833792250...|[0.39085166758450...|       1.0|\n",
      "|[1.0,27.0,6.0,0.0...|      0|[193.138478579040...|[0.38627695715808...|       1.0|\n",
      "|[1.0,27.0,6.0,2.0...|      1|[185.424877645536...|[0.37084975529107...|       1.0|\n",
      "|[1.0,27.0,6.0,3.0...|      0|[164.685852316351...|[0.32937170463270...|       1.0|\n",
      "|[1.0,27.0,9.0,4.0...|      0|[142.006095001922...|[0.28401219000384...|       1.0|\n",
      "|[1.0,32.0,13.0,0....|      1|[176.885399312490...|[0.35377079862498...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[128.585405941664...|[0.25717081188332...|       1.0|\n",
      "|[1.0,32.0,13.0,2....|      1|[126.963464206019...|[0.25392692841203...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[153.429839787005...|[0.30685967957401...|       1.0|\n",
      "|[1.0,32.0,16.5,2....|      1|[153.429839787005...|[0.30685967957401...|       1.0|\n",
      "|[1.0,32.0,16.5,3....|      1|[167.493344562280...|[0.33498668912456...|       1.0|\n",
      "|[1.0,37.0,16.5,1....|      1|[150.090425556233...|[0.30018085111246...|       1.0|\n",
      "|[1.0,37.0,16.5,2....|      1|[154.326506925977...|[0.30865301385195...|       1.0|\n",
      "|[1.0,37.0,23.0,4....|      1|[123.559481897677...|[0.24711896379535...|       1.0|\n",
      "|[1.0,37.0,23.0,5....|      1|[192.317134975430...|[0.38463426995086...|       1.0|\n",
      "|[1.0,42.0,16.5,5....|      1|[151.350101320899...|[0.30270020264179...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[126.937590310819...|[0.25387518062163...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[156.647804216314...|[0.31329560843262...|       1.0|\n",
      "|[1.0,42.0,23.0,2....|      1|[156.647804216314...|[0.31329560843262...|       1.0|\n",
      "+--------------------+-------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_preditions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
